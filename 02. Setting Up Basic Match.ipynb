{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cx-admin\\Documents\\cx-ag\\ag_job_apps\\Turmerik\\TakeHome-ML-SWE\\.venv\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_sci_lg' (0.4.0) was trained with spaCy v3.0.1 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "INFO:nmslib:Loading index from C:\\Users\\cx-admin\\.scispacy\\datasets\\7e8e091ec80370b87b1652f461eae9d926e543a403a69c1f0968f71157322c25.6d801a1e14867953e36258b0e19a23723ae84b0abd2a723bdd3574c3e0c873b4.nmslib_index.bin\n",
      "INFO:nmslib:Loading regular index.\n",
      "INFO:nmslib:Finished loading index\n",
      "INFO:nmslib:Set HNSW query-time parameters:\n",
      "INFO:nmslib:ef(Search)         =20\n",
      "INFO:nmslib:algoType           =2\n",
      "INFO:nmslib:Set HNSW query-time parameters:\n",
      "INFO:nmslib:ef(Search)         =200\n",
      "INFO:nmslib:algoType           =2\n",
      "c:\\Users\\cx-admin\\Documents\\cx-ag\\ag_job_apps\\Turmerik\\TakeHome-ML-SWE\\.venv\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.1.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\cx-admin\\Documents\\cx-ag\\ag_job_apps\\Turmerik\\TakeHome-ML-SWE\\.venv\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.1.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from datetime import datetime\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "from scispacy.linking import EntityLinker\n",
    "\n",
    "# Load SciSpaCy models\n",
    "def load_nlp_models():\n",
    "    \"\"\"\n",
    "    Load and return two NLP models for UMLS linking:\n",
    "    one for conditions and one for medications.\n",
    "    \"\"\"\n",
    "    nlp_umls_link = spacy.load(\"en_core_sci_lg\")\n",
    "    nlp_umls_link.add_pipe(\"abbreviation_detector\")\n",
    "    nlp_umls_link.add_pipe(\"scispacy_linker\", config={\n",
    "        \"resolve_abbreviations\": True,\n",
    "        \"linker_name\": \"umls\"\n",
    "    })\n",
    "\n",
    "    nlp_rxnorm_link = spacy.load(\"en_core_sci_lg\")\n",
    "    nlp_rxnorm_link.add_pipe(\"abbreviation_detector\")\n",
    "    nlp_rxnorm_link.add_pipe(\"scispacy_linker\", config={\n",
    "        \"resolve_abbreviations\": True,\n",
    "        \"linker_name\": \"rxnorm\"\n",
    "    })\n",
    "\n",
    "    return nlp_umls_link, nlp_rxnorm_link\n",
    "\n",
    "# Load NLP models (loading large spacy models for best performance, takes 2-3 minutes)\n",
    "nlp_umls_link, nlp_rxnorm_link = load_nlp_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "from scispacy.linking import EntityLinker\n",
    "\n",
    "def fetch_n_trials(n):\n",
    "    \"\"\"\n",
    "    Fetch 'n' clinical trials from ClinicalTrials.gov that are actively recruiting.\n",
    "    Extract UMLS codes from inclusion and exclusion criteria using provided NLP models.\n",
    "\n",
    "    Parameters:\n",
    "        n (int): Number of trials to fetch.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing trial details and extracted UMLS codes.\n",
    "    \"\"\"\n",
    "    base_url = 'https://clinicaltrials.gov/api/v2/studies'\n",
    "    query_params = {\n",
    "        'format': 'json',\n",
    "        'filter.overallStatus': 'RECRUITING',\n",
    "        'pageSize': n\n",
    "    }\n",
    "\n",
    "    trial_data = []\n",
    "\n",
    "    try:\n",
    "        # Fetch the list of studies\n",
    "        response = requests.get(base_url, params=query_params)\n",
    "        response.raise_for_status()\n",
    "        trials = response.json().get('studies', [])\n",
    "\n",
    "        for trial in trials:\n",
    "            try:\n",
    "                nct_id = trial['protocolSection']['identificationModule']['nctId']\n",
    "                trial_title = trial['protocolSection']['identificationModule']['briefTitle']\n",
    "\n",
    "                # Fetch detailed information for each trial\n",
    "                trial_details_url = f\"{base_url}/{nct_id}?format=json\"\n",
    "                trial_response = requests.get(trial_details_url)\n",
    "                trial_response.raise_for_status()\n",
    "                trial_details = trial_response.json()\n",
    "\n",
    "                # Extract eligibility criteria\n",
    "                eligibility_string = trial_details['protocolSection'].get('eligibilityModule', {}).get('eligibilityCriteria', 'Not Available')\n",
    "                inclusion_criteria, exclusion_criteria = parse_eligibility_criteria(eligibility_string)\n",
    "\n",
    "                # Extract additional trial information\n",
    "                minimum_age = trial_details['protocolSection']['eligibilityModule'].get('minimumAge', 'Not Specified')\n",
    "                maximum_age = trial_details['protocolSection']['eligibilityModule'].get('maximumAge', 'Not Specified')\n",
    "                sex = trial_details['protocolSection']['eligibilityModule'].get('sex', 'Not Specified')\n",
    "                healthy_volunteers = trial_details['protocolSection']['eligibilityModule'].get('healthyVolunteers', 'Not Specified')\n",
    "\n",
    "                # Extract UMLS codes from inclusion and exclusion criteria\n",
    "                inclusion_umls_codes_mapping = extract_codes_from_criteria(inclusion_criteria)\n",
    "                exclusion_umls_codes_mapping = extract_codes_from_criteria(exclusion_criteria)\n",
    "\n",
    "                trial_data.append({\n",
    "                    'NCTId': nct_id,\n",
    "                    'Title': trial_title,\n",
    "                    'Inclusion_Criteria': inclusion_criteria,\n",
    "                    'Exclusion_Criteria': exclusion_criteria,\n",
    "                    'Minimum_Age': minimum_age,\n",
    "                    'Maximum_Age': maximum_age,\n",
    "                    'Sex': sex,\n",
    "                    'Healthy_Volunteers': healthy_volunteers,\n",
    "                    'Inclusion_Criteria_UMLS_Codes': inclusion_umls_codes_mapping,\n",
    "                    'Exclusion_Criteria_UMLS_Codes': exclusion_umls_codes_mapping\n",
    "                })\n",
    "\n",
    "            except KeyError as ke:\n",
    "                print(f\"KeyError: Missing key {ke} in trial data for NCT ID {nct_id}. Skipping this trial.\")\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Error fetching details for trial {nct_id}: {e}\")\n",
    "\n",
    "        # Convert the collected trial data to a pandas DataFrame\n",
    "        df_trials = pd.DataFrame(trial_data)\n",
    "        return df_trials\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching trials: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Compile regex patterns once at the module level\n",
    "CRITERIA_SPLIT_PATTERN = re.compile(r'\\n(?:\\d+\\.\\s|\\*\\s|-|\\u2022|\\u00B7)')\n",
    "INCLUSION_HEADER = \"Inclusion Criteria:\"\n",
    "EXCLUSION_HEADER = \"Exclusion Criteria:\"\n",
    "\n",
    "# Define STOP_CODES as a set for O(1) lookups\n",
    "STOP_CODES = {'C3244316', 'C0013227'}\n",
    "\n",
    "def fetch_n_trials_v2(n=None, max_workers=None, verbose=False, reload=True):\n",
    "    \"\"\"\n",
    "    Fetch 'n' clinical trials from ClinicalTrials.gov that are actively recruiting.\n",
    "    If 'n' is None, fetch all trials that are actively recruiting.\n",
    "    Extract UMLS codes from inclusion and exclusion criteria using provided NLP models.\n",
    "\n",
    "    Parameters:\n",
    "        n (int, optional): Number of trials to fetch. If None, fetch all trials.\n",
    "        max_workers (int, optional): Maximum number of worker threads to use for parallel processing.\n",
    "        verbose (bool, optional): If True, set logging level to INFO. If False, set to WARNING.\n",
    "        reload (bool, optional): If False and processed file exists, load and return it.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing trial details and extracted UMLS codes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the path to save the processed trials\n",
    "    processed_file_path = os.path.join('data', 'trials', 'df_trials_processed.csv')\n",
    "\n",
    "    # If reload is False and the processed file exists, load and return it\n",
    "    if not reload and os.path.exists(processed_file_path):\n",
    "        return pd.read_csv(processed_file_path)\n",
    "\n",
    "    # Setup Logging\n",
    "    logger = logging.getLogger('fetch_n_trials_v2')\n",
    "    logger.setLevel(logging.INFO if verbose else logging.WARNING)\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    if not logger.handlers:\n",
    "        logger.addHandler(handler)\n",
    "\n",
    "    base_url = 'https://clinicaltrials.gov/api/v2/studies'\n",
    "    page_size = 1000  # Adjusted as per your current function\n",
    "    query_params = {\n",
    "        'format': 'json',\n",
    "        'filter.overallStatus': 'RECRUITING',\n",
    "        'pageSize': page_size,\n",
    "        'fields': ','.join([\n",
    "            'protocolSection.identificationModule.nctId',\n",
    "            'protocolSection.identificationModule.briefTitle',\n",
    "            'protocolSection.eligibilityModule.eligibilityCriteria',\n",
    "            'protocolSection.eligibilityModule.minimumAge',\n",
    "            'protocolSection.eligibilityModule.maximumAge',\n",
    "            'protocolSection.eligibilityModule.sex',\n",
    "            'protocolSection.eligibilityModule.healthyVolunteers'\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    trial_data = []\n",
    "    total_fetched = 0\n",
    "    page_token = None\n",
    "    total_trials_processed = 0\n",
    "    save_every = 1000  # Save every 1000 trials\n",
    "    first_save = not os.path.exists(processed_file_path)  # Determine if headers should be written\n",
    "\n",
    "    try:\n",
    "        # Ensure the output directory exists\n",
    "        os.makedirs(os.path.dirname(processed_file_path), exist_ok=True)\n",
    "\n",
    "        logger.info(\"Starting to fetch trials...\")\n",
    "        with tqdm(desc=\"Fetching Trials\", unit=\"page\") as fetch_bar:\n",
    "            while True:\n",
    "                if page_token:\n",
    "                    query_params['pageToken'] = page_token\n",
    "\n",
    "                response = requests.get(base_url, params=query_params)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "\n",
    "                trials = data.get('studies', [])\n",
    "                if not trials:\n",
    "                    logger.info(\"No more trials found.\")\n",
    "                    break\n",
    "\n",
    "                # Limit the number of trials if 'n' is specified\n",
    "                if n is not None:\n",
    "                    remaining = n - total_fetched\n",
    "                    if remaining <= 0:\n",
    "                        logger.info(\"Reached the desired number of trials.\")\n",
    "                        break\n",
    "                    trials = trials[:remaining]\n",
    "\n",
    "                total_fetched += len(trials)\n",
    "                trial_data.extend(trials)\n",
    "\n",
    "                fetch_bar.update(1)\n",
    "\n",
    "                # Check if there is a next page\n",
    "                page_token = data.get('nextPageToken')\n",
    "                if not page_token:\n",
    "                    logger.info(\"No nextPageToken found. All trials fetched.\")\n",
    "                    break\n",
    "\n",
    "                # Stop fetching if we've reached the desired number of trials\n",
    "                if n is not None and total_fetched >= n:\n",
    "                    logger.info(\"Reached the desired number of trials.\")\n",
    "                    break\n",
    "\n",
    "        logger.info(f\"Total trials fetched: {total_fetched}\")\n",
    "\n",
    "        # Process trials in parallel with progress bar\n",
    "        def process_trial(trial):\n",
    "            try:\n",
    "                nct_id = trial['protocolSection']['identificationModule']['nctId']\n",
    "                trial_title = trial['protocolSection']['identificationModule']['briefTitle']\n",
    "\n",
    "                # Extract eligibility criteria\n",
    "                eligibility_string = trial['protocolSection'].get('eligibilityModule', {}).get('eligibilityCriteria', 'Not Available')\n",
    "                inclusion_criteria, exclusion_criteria = parse_eligibility_criteria(eligibility_string)\n",
    "\n",
    "                # Extract additional trial information\n",
    "                eligibility_module = trial['protocolSection'].get('eligibilityModule', {})\n",
    "                minimum_age = eligibility_module.get('minimumAge', 'Not Specified')\n",
    "                maximum_age = eligibility_module.get('maximumAge', 'Not Specified')\n",
    "                sex = eligibility_module.get('sex', 'Not Specified')\n",
    "                healthy_volunteers = eligibility_module.get('healthyVolunteers', 'Not Specified')\n",
    "\n",
    "                # Extract UMLS codes from inclusion and exclusion criteria\n",
    "                inclusion_umls_codes_mapping = extract_codes_from_criteria(inclusion_criteria, nlp_umls_link, nlp_rxnorm_link)\n",
    "                exclusion_umls_codes_mapping = extract_codes_from_criteria(exclusion_criteria, nlp_umls_link, nlp_rxnorm_link)\n",
    "\n",
    "                return {\n",
    "                    'NCTId': nct_id,\n",
    "                    'Title': trial_title,\n",
    "                    'Inclusion_Criteria': inclusion_criteria,\n",
    "                    'Exclusion_Criteria': exclusion_criteria,\n",
    "                    'Minimum_Age': minimum_age,\n",
    "                    'Maximum_Age': maximum_age,\n",
    "                    'Sex': sex,\n",
    "                    'Healthy_Volunteers': healthy_volunteers,\n",
    "                    'Inclusion_Criteria_UMLS_Codes': inclusion_umls_codes_mapping,\n",
    "                    'Exclusion_Criteria_UMLS_Codes': exclusion_umls_codes_mapping\n",
    "                }\n",
    "\n",
    "            except KeyError as ke:\n",
    "                logger.warning(f\"Missing key {ke} in trial data. Skipping trial.\")\n",
    "                return None\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing trial: {e}\")\n",
    "                return None\n",
    "\n",
    "        logger.info(\"Starting parallel processing of trials...\")\n",
    "        save_buffer = []\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = list(executor.submit(process_trial, trial) for trial in trial_data)\n",
    "\n",
    "            # Use tqdm to track progress\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing Trials\", unit=\"trial\"):\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    save_buffer.append(result)\n",
    "                    total_trials_processed += 1\n",
    "\n",
    "                    # Save every 'save_every' trials\n",
    "                    if total_trials_processed % save_every == 0:\n",
    "                        df_save = pd.DataFrame(save_buffer)\n",
    "                        if first_save:\n",
    "                            df_save.to_csv(processed_file_path, mode='w', index=False, encoding='utf-8')\n",
    "                            first_save = False\n",
    "                            logger.info(f\"Saved {save_every} trials to {processed_file_path}.\")\n",
    "                        else:\n",
    "                            df_save.to_csv(processed_file_path, mode='a', index=False, header=False, encoding='utf-8')\n",
    "                            logger.info(f\"Appended {save_every} trials to {processed_file_path}.\")\n",
    "                        save_buffer = []  # Reset buffer\n",
    "\n",
    "        logger.info(f\"Total trials processed successfully: {total_trials_processed}\")\n",
    "\n",
    "        # Save any remaining trials in the buffer\n",
    "        if save_buffer:\n",
    "            df_save = pd.DataFrame(save_buffer)\n",
    "            if first_save:\n",
    "                df_save.to_csv(processed_file_path, mode='w', index=False, encoding='utf-8')\n",
    "                logger.info(f\"Saved remaining {len(save_buffer)} trials to {processed_file_path}.\")\n",
    "            else:\n",
    "                df_save.to_csv(processed_file_path, mode='a', index=False, header=False, encoding='utf-8')\n",
    "                logger.info(f\"Appended remaining {len(save_buffer)} trials to {processed_file_path}.\")\n",
    "\n",
    "        logger.info(\"DataFrame creation and saving successful.\")\n",
    "        return pd.read_csv(processed_file_path)\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f\"Error fetching trials: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_umls_codes(descriptions, nlp):\n",
    "    \"\"\"\n",
    "    Extract UMLS codes from descriptions using the provided NLP model.\n",
    "\n",
    "    Parameters:\n",
    "        descriptions (list): List of text descriptions.\n",
    "        nlp (spacy.lang): NLP model for UMLS linking.\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping from description to list of UMLS codes.\n",
    "    \"\"\"\n",
    "    code_mapping = {}\n",
    "    for desc in descriptions:\n",
    "        if pd.isna(desc) or desc == '':\n",
    "            code_mapping[desc] = []\n",
    "            continue\n",
    "        doc = nlp(desc)\n",
    "        codes = set()\n",
    "        for entity in doc.ents:\n",
    "            for kb_ent in entity._.kb_ents:\n",
    "                concept_id, score = kb_ent\n",
    "                codes.add(concept_id)\n",
    "        code_mapping[desc] = list(codes)\n",
    "    return code_mapping\n",
    "\n",
    "def extract_umls_code_definitions(unique_codes, nlp_umls_conditions=None, nlp_umls_medications=None, reload=False):\n",
    "    \"\"\"\n",
    "    Extract definitions for UMLS codes using SciSpaCy models.\n",
    "\n",
    "    Parameters:\n",
    "        unique_codes (set): Set of unique UMLS codes.\n",
    "        nlp_umls_conditions (spacy.lang): NLP model for UMLS linking (conditions).\n",
    "        nlp_umls_medications (spacy.lang): NLP model for UMLS linking (medications).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing code details.\n",
    "    \"\"\"\n",
    "\n",
    "    # check if present in data/umls_codes.csv\n",
    "    if not reload and os.path.exists('data/umls_codes.csv'):\n",
    "        umls_codes_df = pd.read_csv('data/umls_codes.csv')\n",
    "        umls_codes_df = umls_codes_df[umls_codes_df['code'].isin(unique_codes)]\n",
    "        if len(umls_codes_df) == len(unique_codes):\n",
    "            return umls_codes_df\n",
    "\n",
    "    # Initialize a list to store code information\n",
    "    code_data = []\n",
    "\n",
    "    # Get the linkers from the models\n",
    "    linker_conditions = nlp_umls_conditions.get_pipe('scispacy_linker')\n",
    "    linker_medications = nlp_umls_medications.get_pipe('scispacy_linker')\n",
    "\n",
    "    # Combine the two linkers' knowledge bases\n",
    "    all_linkers = [linker_conditions, linker_medications]\n",
    "    processed_codes = set()\n",
    "\n",
    "    for code in unique_codes:\n",
    "        if code in processed_codes:\n",
    "            continue  # Skip if already processed\n",
    "        entity = None\n",
    "        for linker in all_linkers:\n",
    "            entity = linker.kb.cui_to_entity.get(code)\n",
    "            if entity:\n",
    "                break\n",
    "        if entity:\n",
    "            code_data.append({\n",
    "                'code': code,\n",
    "                'canonical_name': entity.canonical_name,\n",
    "                'definition': entity.definition,\n",
    "                'aliases': entity.aliases,\n",
    "                'types': entity.types\n",
    "            })\n",
    "        else:\n",
    "            # Code not found in either linker\n",
    "            code_data.append({\n",
    "                'code': code,\n",
    "                'canonical_name': None,\n",
    "                'definition': None,\n",
    "                'aliases': None,\n",
    "                'types': None\n",
    "            })\n",
    "        processed_codes.add(code)\n",
    "    \n",
    "    # Create a DataFrame from the code data\n",
    "    spacy_umls_codes_df = pd.DataFrame(code_data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    spacy_umls_codes_df.to_csv('data/umls_codes.csv', index=False)\n",
    "    \n",
    "    return spacy_umls_codes_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"MISTRAL_API_KEY environment variable is not set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "from mistralai import Mistral\n",
    "from mistralai.models.function import Function\n",
    "from mistralai.models.systemmessage import SystemMessage\n",
    "from mistralai.models.usermessage import UserMessage\n",
    "from mistralai.models.toolmessage import ToolMessage\n",
    "\n",
    "# dotenv setup\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "def call_mistral_api(log_text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Call the Mistral AI API with the given log text and retrieve eligibility information.\n",
    "\n",
    "    Parameters:\n",
    "        log_text (str): The log text generated during patient-trial matching.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Parsed JSON response containing 'patientIsEligibleForTrial' (bool),\n",
    "              'inclusionCriteriaMet' (list), and 'exclusionCriteriaMet' (list).\n",
    "    \"\"\"\n",
    "    # Define the function that the model can call\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": Function(\n",
    "                name=\"evaluate_patient_eligibility\",\n",
    "                description=\"Determine if the patient is eligible for the trial based on the log provided.\",\n",
    "                parameters={\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"inclusionCriteriaMet\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Description of the inclusion criterion and why/why not it was met. This explanation should be concise and accurate.\"\n",
    "                            },\n",
    "                            \"description\": \"List of inclusion criteria met by the patient.\"\n",
    "                        },\n",
    "                        \"exclusionCriteriaMet\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Description of the exclusion criterion and why/why not it was met. This explanation should be concise and accurate.\"\n",
    "                            },\n",
    "                            \"description\": \"List of exclusion criteria met by the patient.\"\n",
    "                        },\n",
    "                        \"patientIsEligibleForTrial\": {\n",
    "                            \"type\": \"boolean\",\n",
    "                            \"description\": \"Indicates if the patient is eligible for the trial. This should be an accurate assessment based on the provided log.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"inclusionCriteriaMet\", \"exclusionCriteriaMet\", \"patientIsEligibleForTrial\"]\n",
    "                }\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Initialize Mistral client\n",
    "    api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"MISTRAL_API_KEY environment variable is not set.\")\n",
    "\n",
    "    model = \"mistral-small-latest\"  # Using the cheapest available model\n",
    "\n",
    "    client = Mistral(api_key=api_key)\n",
    "\n",
    "    # Prepare initial messages\n",
    "    messages = [SystemMessage(content=\"\"\"Analyze the following patient-trial matching log and determine eligibility. \n",
    "    **Patient-Trial Matching Log Format:**\n",
    "Patient ID: string\n",
    "Trial ID: string\n",
    "Match Check:\n",
    "1. Age = {{patient_age}} vs {{trial_age_required}} = MATCH (trial age can be nan if not specified)\n",
    "2. Sex = {{patient_sex}} vs {{trial_sex_required}} = MATCH\n",
    "3. Health = {{patient_health_status}} vs {{trial_health_status_required}} = MATCH\n",
    "4. Exclusion Criteria:\n",
    "   - Criterion 1: {{trial_exclusion_criterion_1}} = MATCH (match if patient doesn't have the condition, i.e., MATCH implies the patient is still eligible)\n",
    "    ...\n",
    "   - Criterion n-1: {{trial_exclusion_criterion_n-1}} = NO MATCH (no match if patient has the condition, i.e., NO MATCH implies the patient is not eligible)\n",
    "      + [UMLS_CODE_1] Condition 1 (finding) (UMLS_CODE_1 is the UMLS code for the condition) (these codes and the condition describe why the patient is not eligible)\n",
    "      + [UMLS_CODE_2] Condition 2 (finding) ...\n",
    "   - Criterion n: {{trial_exclusion_criterion_n}} = MATCH \n",
    "\n",
    "    The log contains a simple analysis conducted using SciSpaCy models and their NER capabilities. However, these results may be inconsistent due to false positives. As a Healthcare QC Specialist, you need to review the log and provide a final assessment of the patient's eligibility for the clinical trial.\n",
    "\n",
    "    **Example Output JSON Format:**\n",
    "    {\n",
    "        \"inclusionCriteriaMet\": [\"Patient meets the age criteria.\", ...],\n",
    "        \"exclusionCriteriaMet\": [\"Patient does not have Summarize Condition 1 (UMLS_CODE_1) as required.\", ...],\n",
    "        \"patientIsEligibleForTrial\": true/false\n",
    "    }\n",
    "    \"\"\")]\n",
    "\n",
    "    # Append the log text as user message\n",
    "    messages.append(UserMessage(content=log_text))\n",
    "\n",
    "    # Send the request to Mistral API with tools (which define the output format)\n",
    "    response = client.chat.complete(model=model, messages=messages, tools=tools)\n",
    "\n",
    "    # Handle response and ensure it returns a JSON format with patient eligibility\n",
    "    try:\n",
    "        if response.choices and response.choices[0].message.tool_calls:\n",
    "            # Check if the model invoked the tool and produced the required format\n",
    "            tool_call = response.choices[0].message.tool_calls[0]\n",
    "            function_params = json.loads(tool_call.function.arguments)\n",
    "            return function_params\n",
    "        else:\n",
    "            # Try to parse the response directly as JSON if no tool call\n",
    "            eligibility_json = json.loads(response.choices[0].message.content)\n",
    "            return eligibility_json\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse JSON output from Mistral API.\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, AzureOpenAI\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "from typing import Dict\n",
    "from retry import retry   \n",
    "# dotenv setup to load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def call_openai(log_text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Call the OpenAI API with the given log text and retrieve eligibility information.\n",
    "\n",
    "    Parameters:\n",
    "        log_text (str): The log text generated during patient-trial matching.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Parsed JSON response containing 'patientIsEligibleForTrial' (bool),\n",
    "              'inclusionCriteriaMet' (list), and 'exclusionCriteriaMet' (list).\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Define the function that the model can call\n",
    "    functions = [\n",
    "        {\n",
    "        \"type\": \"function\",\n",
    "        \"function\":\n",
    "            {\n",
    "            \"name\": \"evaluate_patient_eligibility\",\n",
    "            \"description\": \"Determine if the patient is eligible for the trial based on the log provided.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"inclusionCriteriaMet\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Description of the inclusion criterion and why/why not it was met.\"\n",
    "                        },\n",
    "                        \"description\": \"List of inclusion criteria met by the patient.\"\n",
    "                    },\n",
    "                    \"exclusionCriteriaMet\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Description of the exclusion criterion and why/why not it was met.\"\n",
    "                        },\n",
    "                        \"description\": \"List of exclusion criteria met by the patient.\"\n",
    "                    },\n",
    "                    \"patientIsEligibleForTrial\": {\n",
    "                        \"type\": \"boolean\",\n",
    "                        \"description\": \"Indicates if the patient is eligible for the trial.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"inclusionCriteriaMet\", \"exclusionCriteriaMet\", \"patientIsEligibleForTrial\"]\n",
    "            }\n",
    "        }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Prepare the messages for the conversation\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "        Analyze the following patient-trial matching log and determine eligibility. \n",
    "    **Patient-Trial Matching Log Format:**\n",
    "Patient ID: string\n",
    "Trial ID: string\n",
    "Match Check:\n",
    "1. Age = {{patient_age}} vs {{trial_age_required}} = MATCH (trial age can be nan if not specified)\n",
    "2. Sex = {{patient_sex}} vs {{trial_sex_required}} = MATCH\n",
    "3. Health = {{patient_health_status}} vs {{trial_health_status_required}} = MATCH\n",
    "4. Exclusion Criteria:\n",
    "   - Criterion 1: {{trial_exclusion_criterion_1}} = MATCH (match if patient doesn't have the condition, i.e., MATCH implies the patient is still eligible)\n",
    "    ...\n",
    "   - Criterion n-1: {{trial_exclusion_criterion_n-1}} = NO MATCH (no match if patient has the condition, i.e., NO MATCH implies the patient is not eligible)\n",
    "      + [UMLS_CODE_1] Condition 1 (finding) (UMLS_CODE_1 is the UMLS code for the condition) (these codes and the condition describe why the patient is not eligible)\n",
    "      + [UMLS_CODE_2] Condition 2 (finding) ...\n",
    "   - Criterion n: {{trial_exclusion_criterion_n}} = MATCH \n",
    "\n",
    "    The log contains a simple analysis conducted using SciSpaCy models and their NER capabilities. However, these results may be inconsistent due to false positives. As a Healthcare QC Specialist, you need to review the log and provide a final assessment of the patient's eligibility for the clinical trial.\n",
    "\n",
    "    **Example Output JSON Format:**\n",
    "    {\n",
    "        \"inclusionCriteriaMet\": [\"Patient meets the age criteria.\", ...],\n",
    "        \"exclusionCriteriaMet\": [\"Patient does not have Summarize Condition 1 (UMLS_CODE_1) as required.\", ...],\n",
    "        \"patientIsEligibleForTrial\": true/false\n",
    "    }\n",
    "    Note: The \"+ [UMLS_CODE] Condition\" lines could be flawed due to false positives, that's when you need to correct the log. One way to find these false positives is to check if description of criteria n matches the conditions described by the codes. If not, it is a false positive.\n",
    "    Finally, make modifications to the log (if necessary) to ensure the patient's eligibility is accurately assessed. Watch out for false positives and negatives in the criteria analysis. Use chain-of-thought reasoning to spot double negatives and other complex patterns.\n",
    "        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": log_text}\n",
    "    ]\n",
    "\n",
    "    # Call the OpenAI API\n",
    "    response_json = call_openai_api(\n",
    "        model=\"gpt-4o\",\n",
    "        seed=None,\n",
    "        bw_message=messages,\n",
    "        tools=functions,\n",
    "        tool_choice= {\"type\": \"function\", \"function\": {\"name\": \"evaluate_patient_eligibility\"}},\n",
    "        temperature=0.5,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    return response_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(tries=3, delay=2)  \n",
    "def call_openai_api(  \n",
    "        model: str,\n",
    "        seed: int,  \n",
    "        bw_message: list,  \n",
    "        tools: list = None,\n",
    "        tool_choice: dict = None,  \n",
    "        temperature: float = 0,  \n",
    "        max_tokens: int = 500\n",
    "    ):  \n",
    "    try:\n",
    "        # client = OpenAI()\n",
    "        client = AzureOpenAI(\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "            api_version=\"2024-07-01-preview\",\n",
    "            azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        )\n",
    "\n",
    "        response = client.chat.completions.create(  \n",
    "            model=model,  \n",
    "            seed=seed,  \n",
    "            messages=bw_message,  \n",
    "            temperature=temperature,  \n",
    "            max_tokens=max_tokens,  \n",
    "            top_p=1,  \n",
    "            frequency_penalty=0,  \n",
    "            presence_penalty=0,  \n",
    "            tools=tools,  \n",
    "            tool_choice=tool_choice,  \n",
    "        )  \n",
    "\n",
    "        if tool_choice is None:\n",
    "            return response.choices[0].message.content\n",
    "        else:\n",
    "            output = response.choices[0].message.tool_calls[0].function.arguments  \n",
    "  \n",
    "        output_dict = json.loads(output)\n",
    "        return output_dict        \n",
    "          \n",
    "    except openai.APIError as e:  \n",
    "        print(f\"\\t- OpenAI API returned an API Error: {e}\")  \n",
    "        raise  \n",
    "    except openai.APIConnectionError as e:  \n",
    "        print(f\"\\t- OpenAI API connection error: {e}\")  \n",
    "        raise  \n",
    "    except openai.RateLimitError as e:  \n",
    "        print(f\"\\t- OpenAI API rate limit error: {e}\")  \n",
    "        raise  \n",
    "    except Exception as e:  \n",
    "        print(f\"\\t- An error occurred: {e}\")  \n",
    "        raise  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'inclusionCriteriaMet': ['Patient meets the age criteria.',\n",
       "  'Patient meets the sex criteria.'],\n",
       " 'exclusionCriteriaMet': ['Patient does not have deformity and pathology in the shoulder region as required.',\n",
       "  'Patient does not have known local anesthetic allergy as required.',\n",
       "  'Patient does not have BMI>35 as required.',\n",
       "  'Patient has alcohol and substance addiction, which does not meet the exclusion criteria.',\n",
       "  'Patient cannot perceive and evaluate pain due to psychiatric illness, which does not meet the exclusion criteria.'],\n",
       " 'patientIsEligibleForTrial': False}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_log_text = \"\"\"\n",
    "Patient ID: f264901f-b031-8e53-26f0-92ca046b9472\n",
    "Trial ID: NCT05666076\n",
    "Match Check:\n",
    "1. Age = 55 vs [nan, nan] = MATCH\n",
    "2. Sex = M vs ALL = MATCH\n",
    "3. Health = Not Healthy vs Patients = MATCH\n",
    "4. Exclusion Criteria:\n",
    "   - Criterion 1: Patients with deformity and pathology in the shoulder region = MATCH\n",
    "   - Criterion 2: Patients with known local anesthetic allergy = MATCH\n",
    "   - Criterion 3: Patients with BMI\\>35 = MATCH\n",
    "   - Criterion 4: Patients with alcohol and substance addiction = NO MATCH\n",
    "      + [C0001948] Unhealthy alcohol drinking behavior (finding)\n",
    "      + [C0001962] Unhealthy alcohol drinking behavior (finding)\n",
    "   - Criterion 5: Patients with opioid addiction = MATCH\n",
    "   - Criterion 6: Patients who cannot perceive and evaluate pain, such as psychiatric illness, mental retardation = NO MATCH\n",
    "      + [C0004936] Ischemic heart disease (disorder), Acute bronchitis (disorder), Acute viral pharyngitis (disorder), Gingivitis (disorder), Viral sinusitis (disorder)\n",
    "\"\"\"\n",
    "\n",
    "# call_mistral_api(sample_log_text)\n",
    "call_openai(sample_log_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for data loading and processing\n",
    "def load_data(file_path, encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    Load data from a CSV file with the specified encoding.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the CSV file to be loaded.\n",
    "    encoding (str, optional): The encoding used to read the CSV file. Default is 'utf-8'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A pandas DataFrame containing the loaded data.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path, encoding=encoding)\n",
    "\n",
    "# Function to calculate age in years based on birthdate.\n",
    "def calculate_age(birthdate):\n",
    "    \"\"\"\n",
    "    Calculate age in years based on the provided birthdate.\n",
    "\n",
    "    Args:\n",
    "        birthdate (datetime): The birthdate of the individual.\n",
    "\n",
    "    Returns:\n",
    "        int: The age in years.\n",
    "    \"\"\"\n",
    "    current_date = datetime.now()\n",
    "    return (current_date - birthdate).days // 365\n",
    "\n",
    "def convert_to_datetime(df, date_columns):\n",
    "    \"\"\"\n",
    "    Convert specified columns in DataFrame to datetime.\n",
    "    \"\"\"\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def extract_umls_codes(descriptions, nlp):\n",
    "    \"\"\"\n",
    "    Extract UMLS codes from descriptions using the provided NLP model.\n",
    "    Returns a mapping from description to list of UMLS codes.\n",
    "    \"\"\"\n",
    "    code_mapping = {}\n",
    "    for desc in descriptions:\n",
    "        if pd.isna(desc) or desc == '':\n",
    "            code_mapping[desc] = []\n",
    "            continue\n",
    "        doc = nlp(desc)\n",
    "        codes = set()\n",
    "        for entity in doc.ents:\n",
    "            for kb_ent in entity._.kb_ents:\n",
    "                concept_id, score = kb_ent\n",
    "                codes.add(concept_id)\n",
    "        code_mapping[desc] = list(codes)\n",
    "    return code_mapping\n",
    "\n",
    "def aggregate_list(series):\n",
    "    \"\"\"\n",
    "    Aggregate a pandas Series into a list of unique, non-null values.\n",
    "    \"\"\"\n",
    "    return list(series.dropna().unique())\n",
    "\n",
    "def aggregate_codes(series):\n",
    "    \"\"\"\n",
    "    Aggregate a pandas Series of lists into a list of unique codes.\n",
    "    \"\"\"\n",
    "    codes = set()\n",
    "    for items in series.dropna():\n",
    "        codes.update(items)\n",
    "    return list(codes)\n",
    "\n",
    "def create_code_description_map(codes_list, descriptions_list):\n",
    "    \"\"\"\n",
    "    Create a mapping from codes to descriptions.\n",
    "    \"\"\"\n",
    "    code_desc_map = {}\n",
    "    for codes, desc in zip(codes_list, descriptions_list):\n",
    "        if isinstance(codes, list):\n",
    "            for code in codes:\n",
    "                if code in code_desc_map:\n",
    "                    code_desc_map[code].add(desc)\n",
    "                else:\n",
    "                    code_desc_map[code] = {desc}\n",
    "    return code_desc_map\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "def prepare_patient_data(reload=False):\n",
    "    \"\"\"\n",
    "    Load and preprocess patient data, conditions, and medications.\n",
    "    Returns the patient_profiles_grouped DataFrame.\n",
    "    \"\"\"\n",
    "    STOP_CODES = {'C3244316', 'C0013227'}\n",
    "    if not reload and os.path.exists('data/patient/patient_profiles_grouped.csv'):\n",
    "        return pd.read_csv('data/patient/patient_profiles_grouped.csv')\n",
    "        \n",
    "    # Load data\n",
    "    patients_df = load_data('synthea_100/patients.csv')\n",
    "    medications_df = load_data('synthea_100/medications.csv')\n",
    "    conditions_df = load_data('synthea_100/conditions.csv')\n",
    "\n",
    "    # Calculate age\n",
    "    patients_df['BIRTHDATE'] = pd.to_datetime(patients_df['BIRTHDATE'], errors='coerce')\n",
    "    patients_df['AGE'] = patients_df['BIRTHDATE'].apply(calculate_age)\n",
    "\n",
    "    # Convert date columns to datetime\n",
    "    date_columns_conditions = ['START', 'STOP']\n",
    "    date_columns_medications = ['START', 'STOP']\n",
    "    conditions_df = convert_to_datetime(conditions_df, date_columns_conditions)\n",
    "    medications_df = convert_to_datetime(medications_df, date_columns_medications)\n",
    "\n",
    "\n",
    "    # Extract UMLS codes for conditions\n",
    "    unique_condition_descriptions = conditions_df['DESCRIPTION'].unique()\n",
    "    condition_description_to_umls_codes = extract_umls_codes(unique_condition_descriptions, nlp_umls_link)\n",
    "    conditions_df['Condition_UMLS_CODES'] = conditions_df['DESCRIPTION'].map(condition_description_to_umls_codes)\n",
    "\n",
    "    # Extract UMLS codes for medications\n",
    "    unique_medication_descriptions = medications_df['DESCRIPTION'].unique()\n",
    "    medication_description_to_umls_codes = extract_umls_codes(unique_medication_descriptions, nlp_rxnorm_link)\n",
    "    medications_df['Medication_UMLS_CODES'] = medications_df['DESCRIPTION'].map(medication_description_to_umls_codes)\n",
    "\n",
    "    conditions_df['Condition_UMLS_CODES'] = conditions_df['Condition_UMLS_CODES'].apply(\n",
    "        lambda codes: [code for code in codes if code not in STOP_CODES]\n",
    "    )\n",
    "\n",
    "    medications_df['Medication_UMLS_CODES'] = medications_df['Medication_UMLS_CODES'].apply(\n",
    "        lambda codes: [code for code in codes if code not in STOP_CODES]\n",
    "    )\n",
    "\n",
    "    # Merge DataFrames\n",
    "    patients_conditions = pd.merge(patients_df, conditions_df, left_on='Id', right_on='PATIENT', how='left')\n",
    "    conditions_medications = pd.merge(patients_conditions, medications_df, left_on='ENCOUNTER', right_on='ENCOUNTER', how='left')\n",
    "\n",
    "    # Create Patient Profiles\n",
    "    patient_profiles = conditions_medications[['Id', 'AGE', 'GENDER', \n",
    "                                               'DESCRIPTION_x', 'CODE_x', 'START_x', 'STOP_x', 'Condition_UMLS_CODES',\n",
    "                                               'DESCRIPTION_y', 'CODE_y', 'START_y', 'STOP_y', 'Medication_UMLS_CODES',\n",
    "                                               'ENCOUNTER']]\n",
    "\n",
    "    # Rename columns\n",
    "    patient_profiles.columns = [\n",
    "        'PatientID', 'Age', 'Gender', \n",
    "        'Condition_Description', 'Condition_Code', 'Condition_Start', 'Condition_End', 'Condition_UMLS_CODES',\n",
    "        'Medication_Description', 'Medication_Code', 'Medication_Start', 'Medication_End', 'Medication_UMLS_CODES',\n",
    "        'Encounter'\n",
    "    ]\n",
    "\n",
    "    # Create Patient Profiles Grouped\n",
    "    patient_data_list = []\n",
    "    patient_groups = patient_profiles.groupby('PatientID')\n",
    "\n",
    "    for patient_id, group in patient_groups:\n",
    "        age = group['Age'].iloc[0]\n",
    "        gender = group['Gender'].iloc[0]\n",
    "        condition_descriptions = aggregate_list(group['Condition_Description'])\n",
    "        condition_codes = aggregate_list(group['Condition_Code'])\n",
    "        condition_umls_codes = aggregate_codes(group['Condition_UMLS_CODES'])\n",
    "        medication_descriptions = aggregate_list(group['Medication_Description'])\n",
    "        medication_codes = aggregate_list(group['Medication_Code'])\n",
    "        medication_umls_codes = aggregate_codes(group['Medication_UMLS_CODES'])\n",
    "\n",
    "        # Create code-description maps\n",
    "        condition_code_desc_map = create_code_description_map(group['Condition_UMLS_CODES'], group['Condition_Description'])\n",
    "        medication_code_desc_map = create_code_description_map(group['Medication_UMLS_CODES'], group['Medication_Description'])\n",
    "\n",
    "        patient_data_list.append({\n",
    "            'PatientID': patient_id,\n",
    "            'Age': age,\n",
    "            'Gender': gender,\n",
    "            'Condition_Description': condition_descriptions,\n",
    "            'Condition_Code': condition_codes,\n",
    "            'Condition_UMLS_CODES': condition_umls_codes,\n",
    "            'Condition_Code_Description_Map': condition_code_desc_map,\n",
    "            'Medication_Description': medication_descriptions,\n",
    "            'Medication_Code': medication_codes,\n",
    "            'Medication_UMLS_CODES': medication_umls_codes,\n",
    "            'Medication_Code_Description_Map': medication_code_desc_map\n",
    "        })\n",
    "\n",
    "    patient_profiles_grouped = pd.DataFrame(patient_data_list)\n",
    "\n",
    "    # save both dataframes to csv\n",
    "    patient_profiles_grouped.to_csv('data/patient/patient_profiles_grouped.csv', index=False)\n",
    "    conditions_medications.to_csv('data/annotated_synthea/conditions_medications.csv', index=False)\n",
    "\n",
    "    return patient_profiles_grouped\n",
    "\n",
    "def is_healthy_volunteer(patient):\n",
    "    \"\"\"\n",
    "    Determine if a patient is a healthy volunteer.\n",
    "    \"\"\"\n",
    "    return len(patient['Condition_Description']) == 0\n",
    "\n",
    "def perform_exclusion_criteria_check(patient, trial):\n",
    "    \"\"\"\n",
    "    Check exclusion criteria and return match status and details.\n",
    "    \"\"\"\n",
    "    exclusion_match = True  # Assume match until proven otherwise\n",
    "    exclusion_logs = []\n",
    "\n",
    "    # Prepare patient's condition and medication codes\n",
    "    patient_condition_codes = set(patient['Condition_UMLS_CODES'])\n",
    "    patient_medication_codes = set(patient['Medication_UMLS_CODES'])\n",
    "\n",
    "    # Iterate over exclusion criteria\n",
    "    for idx, crit in enumerate(trial['Exclusion_Criteria']):\n",
    "        criterion_match = True  # Assume criterion matches\n",
    "        criterion_logs = [f\"   - Criterion {idx+1}: {crit}\"]\n",
    "\n",
    "        # Get codes associated with the criterion\n",
    "        criterion_umls_codes = set(trial['Exclusion_Criteria_UMLS_Codes'].get(crit, []))\n",
    "\n",
    "        # Check condition codes\n",
    "        overlapping_condition_codes = patient_condition_codes.intersection(criterion_umls_codes)\n",
    "        if overlapping_condition_codes:\n",
    "            criterion_match = False\n",
    "            for code in overlapping_condition_codes:\n",
    "                descriptions = patient['Condition_Code_Description_Map'].get(code, [])\n",
    "                descriptions_str = ', '.join(descriptions)\n",
    "                criterion_logs.append(f\"      + [{code}] {descriptions_str}\")\n",
    "        \n",
    "        # Check medication codes\n",
    "        overlapping_medication_codes = patient_medication_codes.intersection(criterion_umls_codes)\n",
    "        if overlapping_medication_codes:\n",
    "            criterion_match = False\n",
    "            for code in overlapping_medication_codes:\n",
    "                descriptions = patient['Medication_Code_Description_Map'].get(code, [])\n",
    "                descriptions_str = ', '.join(descriptions)\n",
    "                criterion_logs.append(f\"      + [{code}] {descriptions_str}\")\n",
    "\n",
    "        # Append match status for the criterion\n",
    "        criterion_logs[0] += f\" = {'MATCH' if criterion_match else 'NO MATCH'}\"\n",
    "        if not criterion_match:\n",
    "            exclusion_match = False\n",
    "\n",
    "        exclusion_logs.extend(criterion_logs)\n",
    "\n",
    "    return exclusion_match, exclusion_logs\n",
    "\n",
    "def match_patients_to_trials(\n",
    "        patient_profiles_grouped,\n",
    "        df_trials,\n",
    "        code_definitions_df,\n",
    "        chunk_size=100,\n",
    "        debug=False,\n",
    "        verbose=True,\n",
    "        max_api_workers=10,\n",
    "        output_json_format='both',\n",
    "        patient_start_chunk=0,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Match patients to clinical trials based on eligibility criteria, with API integration.\n",
    "\n",
    "    Parameters:\n",
    "        patient_profiles_grouped (pd.DataFrame): DataFrame containing patient profiles.\n",
    "        df_trials (pd.DataFrame): DataFrame containing trial information.\n",
    "        code_definitions_df (pd.DataFrame): DataFrame with code definitions.\n",
    "        chunk_size (int): Number of patients and trials to process per chunk.\n",
    "        debug (bool): If True, generate detailed logs for each patient-trial match.\n",
    "        verbose (bool): If True, logging and progress bars are enabled.\n",
    "        max_api_workers (int): Maximum number of concurrent API calls.\n",
    "        output_json_format (str): Options to include in output ('match', 'no_match', 'both').\n",
    "        patient_start_chunk (int): Index of the starting patient chunk.\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries containing patient IDs and their eligible trials.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up logging to the specified log file\n",
    "    datetime_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_filename = f\"data/output/pt_matches_progress_{datetime_str}.log\"\n",
    "\n",
    "    # Set up the logger for use in Jupyter\n",
    "    logger = setup_logger_for_jupyter(log_filename, debug=debug)\n",
    "\n",
    "    if not verbose:\n",
    "        logger.disabled = True\n",
    "\n",
    "    # Initialize tqdm with pandas\n",
    "    tqdm.pandas()\n",
    "\n",
    "    # Prepare trial data\n",
    "    df_trials = prepare_trial_data(df_trials)\n",
    "    logger.info(\"Trial data prepared.\")\n",
    "\n",
    "    # Initialize the final results list\n",
    "    final_results = []\n",
    "\n",
    "    # Mapping to store match results for each patient-trial pair\n",
    "    match_results_mapping = {}\n",
    "\n",
    "    patient_chunk_size = 50\n",
    "\n",
    "    # Process patients in chunks with progress bar\n",
    "    for patient_start in tqdm(range(patient_start_chunk, len(patient_profiles_grouped), patient_chunk_size), desc=\"Patient Chunks\"):\n",
    "        patient_chunk = patient_profiles_grouped.iloc[patient_start:patient_start + patient_chunk_size]\n",
    "        logger.info(f\"Processing patient chunk starting at index {patient_start}\")\n",
    "\n",
    "        # Process trials in chunks with progress bar\n",
    "        for trial_start in tqdm(range(0, len(df_trials), chunk_size), desc=\"Trial Chunks\", leave=False):\n",
    "            trial_chunk = df_trials.iloc[trial_start:trial_start + chunk_size]\n",
    "            logger.info(f\"Processing trial chunk starting at index {trial_start}\")\n",
    "\n",
    "            # Create cartesian product and apply matching function\n",
    "            patient_trial_pairs = patient_chunk.assign(key=1).merge(\n",
    "                trial_chunk.assign(key=1), on='key').drop('key', axis=1)\n",
    "\n",
    "            # Apply matching function with progress bar\n",
    "            match_results = patient_trial_pairs.progress_apply(\n",
    "                lambda row: evaluate_patient_trial_match_w_api(\n",
    "                    row, code_definitions_df=code_definitions_df, debug=debug), axis=1)\n",
    "\n",
    "            patient_trial_pairs['Match_Result'] = match_results\n",
    "\n",
    "            # Collect results and prepare API call tasks\n",
    "            for idx, row in patient_trial_pairs.iterrows():\n",
    "                patient_id = row['PatientID']\n",
    "                trial_id = row['NCTId']\n",
    "                match_result = row['Match_Result']\n",
    "\n",
    "                # Store match result for later use\n",
    "                match_results_mapping[(patient_id, trial_id)] = match_result\n",
    "\n",
    "                if match_result['is_match']:\n",
    "                    # Prepare initial eligible trial entry\n",
    "                    trial_entry = {\n",
    "                        'trialId': trial_id,\n",
    "                        'trialName': row['Title'],\n",
    "                        'eligibilityCriteriaMet': match_result['eligibilityCriteriaMet']\n",
    "                    }\n",
    "\n",
    "                    # Add to final results\n",
    "                    existing_patient = next((item for item in final_results if item['patientId'] == patient_id), None)\n",
    "                    if existing_patient:\n",
    "                        existing_patient['eligibleTrials'].append(trial_entry)\n",
    "                    else:\n",
    "                        final_results.append({\n",
    "                            'patientId': patient_id,\n",
    "                            'eligibleTrials': [trial_entry]\n",
    "                        })\n",
    "\n",
    "        # Save results after processing each patient chunk\n",
    "        output_file_path = f'data/output/patient_trial_matches_chunk_{patient_start}.json'\n",
    "        save_results_to_json(final_results, output_file_path, logger)\n",
    "\n",
    "        # Clear final_results to free memory\n",
    "        final_results.clear()\n",
    "\n",
    "    logger.info(\"All patient chunks processed. Returning final results.\")\n",
    "    return final_results\n",
    "\n",
    "def save_results_to_json(results, output_file_path, logger):\n",
    "    \"\"\"\n",
    "    Save the results to a JSON file, appending if the file already exists.\n",
    "\n",
    "    Parameters:\n",
    "        results (list): The results to save.\n",
    "        output_file_path (str): The path to the output JSON file.\n",
    "        logger (logging.Logger): The logger instance.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Save the results to the JSON file\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "\n",
    "        logger.info(f\"Results saved to {output_file_path}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save results to {output_file_path}: {e}\")\n",
    "\n",
    "def update_logs_and_format_output(final_results, api_results, match_results_mapping, output_json_format='both', debug=False):\n",
    "    \"\"\"\n",
    "    Update logs based on API results and format the final JSON output.\n",
    "\n",
    "    Parameters:\n",
    "        final_results (list): List of dictionaries containing patient IDs and their eligible trials.\n",
    "        api_results (dict): Dictionary mapping (patient_id, trial_id) to API output.\n",
    "        match_results_mapping (dict): Dictionary mapping (patient_id, trial_id) to initial match results.\n",
    "        output_json_format (str): Options to include in output ('match', 'no_match', 'both').\n",
    "        debug (bool): If True, additional debug information will be logged.\n",
    "\n",
    "    Returns:\n",
    "        list: Updated list of dictionaries containing patient IDs and their eligible trials.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import re\n",
    "    from glob import glob\n",
    "\n",
    "    updated_final_results = []\n",
    "\n",
    "    for patient_entry in final_results:\n",
    "        patient_id = patient_entry['patientId']\n",
    "        updated_eligible_trials = []\n",
    "\n",
    "        for trial_entry in patient_entry['eligibleTrials']:\n",
    "            trial_id = trial_entry['trialId']\n",
    "            key = (patient_id, trial_id)\n",
    "\n",
    "            # Get initial match result\n",
    "            initial_match_result = match_results_mapping.get(key)\n",
    "            initial_is_match = initial_match_result.get('is_match') if initial_match_result else None\n",
    "\n",
    "            if key in api_results:\n",
    "                api_result = api_results[key]\n",
    "\n",
    "                # Update 'eligibilityCriteriaMet' with concatenated lists from API output\n",
    "                eligibility_criteria_met = (\n",
    "                    api_result.get('inclusionCriteriaMet', []) +\n",
    "                    api_result.get('exclusionCriteriaMet', [])\n",
    "                )\n",
    "                trial_entry['eligibilityCriteriaMet'] = eligibility_criteria_met\n",
    "\n",
    "                # Determine if the patient is eligible based on API result\n",
    "                patient_is_eligible = api_result.get('patientIsEligibleForTrial', False)\n",
    "\n",
    "                # Update log file\n",
    "                log_dir = os.path.join('logs', patient_id)\n",
    "                # Find the original log file\n",
    "                log_files_pattern = os.path.join(log_dir, f\"*{trial_id}_*.log\")\n",
    "                log_files = glob(log_files_pattern)\n",
    "                if log_files:\n",
    "                    original_log_filepath = log_files[0]\n",
    "                    # Read the original log content\n",
    "                    with open(original_log_filepath, 'r', encoding='utf-8') as f:\n",
    "                        log_content = f.read()\n",
    "\n",
    "                    # Append the API response to the logs\n",
    "                    api_check_lines = [\"API Check:\"]\n",
    "                    # Pretty-print the API response\n",
    "                    for key_resp, value_resp in api_result.items():\n",
    "                        if isinstance(value_resp, list):\n",
    "                            api_check_lines.append(f\"{key_resp}:\")\n",
    "                            for item in value_resp:\n",
    "                                api_check_lines.append(f\"\\t- {item}\")\n",
    "                        else:\n",
    "                            api_check_lines.append(f\"{key_resp}: {value_resp}\")\n",
    "                    api_check_text = '\\n'.join(api_check_lines)\n",
    "                    log_content += '\\n' + api_check_text\n",
    "\n",
    "                    # Determine if file name needs to be updated\n",
    "                    initial_status = 'MATCH' if initial_is_match else 'NO_MATCH'\n",
    "                    updated_status = 'MATCH' if patient_is_eligible else 'NO_MATCH'\n",
    "\n",
    "                    if initial_status != updated_status:\n",
    "                        # Remove the initial status prefix from the filename\n",
    "                        original_log_filename = os.path.basename(original_log_filepath)\n",
    "                        original_log_filename = re.sub(r'^\\[.*?\\]_','', original_log_filename)\n",
    "                        # Construct new file name with updated status\n",
    "                        new_log_filename = f'[{updated_status}]_{original_log_filename}'\n",
    "                        new_log_filepath = os.path.join(log_dir, new_log_filename)\n",
    "                        # Rename the file\n",
    "                        os.rename(original_log_filepath, new_log_filepath)\n",
    "                        # Write the updated log content to the renamed file\n",
    "                        with open(new_log_filepath, 'w', encoding='utf-8') as f:\n",
    "                            f.write(log_content)\n",
    "                    else:\n",
    "                        # Status didn't change, overwrite the original file\n",
    "                        with open(original_log_filepath, 'w', encoding='utf-8') as f:\n",
    "                            f.write(log_content)\n",
    "                else:\n",
    "                    # Log file not found; handle this case if necessary\n",
    "                    if debug:\n",
    "                        print(f\"Log file not found for patient {patient_id} and trial {trial_id}.\")\n",
    "\n",
    "                # Decide whether to include this trial based on 'output_json_format'\n",
    "                if output_json_format == 'both':\n",
    "                    updated_eligible_trials.append(trial_entry)\n",
    "                elif output_json_format == 'match' and patient_is_eligible:\n",
    "                    updated_eligible_trials.append(trial_entry)\n",
    "                elif output_json_format == 'no_match' and not patient_is_eligible:\n",
    "                    updated_eligible_trials.append(trial_entry)\n",
    "                # If not matching the criteria, do not include\n",
    "\n",
    "            else:\n",
    "                # No API result for this trial; include based on initial match\n",
    "                # Get initial match status\n",
    "                if initial_is_match is not None:\n",
    "                    if output_json_format == 'both':\n",
    "                        updated_eligible_trials.append(trial_entry)\n",
    "                    elif output_json_format == 'match' and initial_is_match:\n",
    "                        updated_eligible_trials.append(trial_entry)\n",
    "                    elif output_json_format == 'no_match' and not initial_is_match:\n",
    "                        updated_eligible_trials.append(trial_entry)\n",
    "\n",
    "        if updated_eligible_trials:\n",
    "            updated_final_results.append({\n",
    "                'patientId': patient_id,\n",
    "                'eligibleTrials': updated_eligible_trials\n",
    "            })\n",
    "\n",
    "    return updated_final_results\n",
    "\n",
    "def setup_logger_for_jupyter(log_filename, debug=False):\n",
    "    \"\"\"\n",
    "    Sets up the logger to log into a file in a Jupyter environment.\n",
    "    Removes existing handlers to avoid conflicts.\n",
    "    \"\"\"\n",
    "    # Create a logger\n",
    "    logger = logging.getLogger('match_patients_to_trials_w_api')\n",
    "\n",
    "    # Clear existing handlers to prevent multiple outputs in Jupyter\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    # Set logging level\n",
    "    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n",
    "\n",
    "    # Create a file handler which logs even debug messages\n",
    "    fh = logging.FileHandler(log_filename)\n",
    "    fh.setLevel(logging.DEBUG if debug else logging.INFO)\n",
    "\n",
    "    # Create formatter and add it to the handlers\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    fh.setFormatter(formatter)\n",
    "\n",
    "    # Add the file handler to the logger (logging only to the file now)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    return logger\n",
    "\n",
    "def match_patients_to_trials_w_api(\n",
    "        patient_profiles_grouped,\n",
    "        df_trials,\n",
    "        code_definitions_df,\n",
    "        chunk_size=100,\n",
    "        debug=False,\n",
    "        verbose=True,\n",
    "        max_api_workers=10,\n",
    "        output_json_format='both'\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Match patients to clinical trials based on eligibility criteria, with API integration.\n",
    "\n",
    "    Parameters:\n",
    "        patient_profiles_grouped (pd.DataFrame): DataFrame containing patient profiles.\n",
    "        df_trials (pd.DataFrame): DataFrame containing trial information.\n",
    "        code_definitions_df (pd.DataFrame): DataFrame with code definitions.\n",
    "        chunk_size (int): Number of patients and trials to process per chunk.\n",
    "        debug (bool): If True, generate detailed logs for each patient-trial match.\n",
    "        verbose (bool): If True, logging and progress bars are enabled.\n",
    "        max_api_workers (int): Maximum number of concurrent API calls.\n",
    "        output_json_format (str): Options to include in output ('match', 'no_match', 'both').\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries containing patient IDs and their eligible trials.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up logging to the specified log file\n",
    "    datetime_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_filename = f\"data/output/pt_matches_progress_{datetime_str}.log\"\n",
    "\n",
    "    # Set up the logger for use in Jupyter\n",
    "    logger = setup_logger_for_jupyter(log_filename, debug=debug)\n",
    "\n",
    "    if not verbose:\n",
    "        logger.disabled = True\n",
    "\n",
    "    # Initialize tqdm with pandas\n",
    "    tqdm.pandas()\n",
    "\n",
    "    # Prepare trial data\n",
    "    df_trials = prepare_trial_data(df_trials)\n",
    "    logger.info(\"Trial data prepared.\")\n",
    "\n",
    "    # Initialize the final results list\n",
    "    final_results = []\n",
    "\n",
    "    # Initialize list to collect API call tasks\n",
    "    api_call_tasks = []\n",
    "\n",
    "    # Mapping to store match results for each patient-trial pair\n",
    "    match_results_mapping = {}\n",
    "\n",
    "    patient_chunk_size = 50\n",
    "\n",
    "    # Process patients in chunks with progress bar\n",
    "    for patient_start in tqdm(range(0, len(patient_profiles_grouped), patient_chunk_size), desc=\"Patient Chunks\"):\n",
    "        patient_chunk = patient_profiles_grouped.iloc[patient_start:patient_start + patient_chunk_size]\n",
    "        logger.info(f\"Processing patient chunk starting at index {patient_start}\")\n",
    "\n",
    "        # Process trials in chunks with progress bar\n",
    "        for trial_start in tqdm(range(0, len(df_trials), chunk_size), desc=\"Trial Chunks\", leave=False):\n",
    "            trial_chunk = df_trials.iloc[trial_start:trial_start + chunk_size]\n",
    "            logger.info(f\"Processing trial chunk starting at index {trial_start}\")\n",
    "\n",
    "            # Create cartesian product and apply matching function\n",
    "            patient_trial_pairs = patient_chunk.assign(key=1).merge(\n",
    "                trial_chunk.assign(key=1), on='key').drop('key', axis=1)\n",
    "\n",
    "            # Apply matching function with progress bar\n",
    "            match_results = patient_trial_pairs.progress_apply(\n",
    "                lambda row: evaluate_patient_trial_match_w_api(\n",
    "                    row, code_definitions_df=code_definitions_df, debug=debug), axis=1)\n",
    "\n",
    "            patient_trial_pairs['Match_Result'] = match_results\n",
    "\n",
    "            # Collect results and prepare API call tasks\n",
    "            for idx, row in patient_trial_pairs.iterrows():\n",
    "                patient_id = row['PatientID']\n",
    "                trial_id = row['NCTId']\n",
    "                match_result = row['Match_Result']\n",
    "\n",
    "                # Store match result for later use\n",
    "                match_results_mapping[(patient_id, trial_id)] = match_result\n",
    "\n",
    "                if match_result['is_match']:\n",
    "                    # Prepare initial eligible trial entry\n",
    "                    trial_entry = {\n",
    "                        'trialId': trial_id,\n",
    "                        'trialName': row['Title'],\n",
    "                        'eligibilityCriteriaMet': match_result['eligibilityCriteriaMet']\n",
    "                    }\n",
    "\n",
    "                    # Add to final results\n",
    "                    existing_patient = next((item for item in final_results if item['patientId'] == patient_id), None)\n",
    "                    if existing_patient:\n",
    "                        existing_patient['eligibleTrials'].append(trial_entry)\n",
    "                    else:\n",
    "                        final_results.append({\n",
    "                            'patientId': patient_id,\n",
    "                            'eligibleTrials': [trial_entry]\n",
    "                        })\n",
    "\n",
    "                # Check if API call is needed\n",
    "                if match_result['need_api_call']:\n",
    "                    # Prepare arguments for the API call\n",
    "                    log_text = match_result['log_text']\n",
    "                    # Enqueue API call\n",
    "                    api_call_tasks.append((patient_id, trial_id, log_text))\n",
    "\n",
    "        output_file_path = 'data/output/patient_trial_matches.json'\n",
    "        save_results_to_json(final_results, output_file_path, logger)\n",
    "\n",
    "    # Log the number of API calls to be made\n",
    "    total_api_calls = len(api_call_tasks)\n",
    "    logger.info(f\"Total API calls to be made: {total_api_calls}\")\n",
    "\n",
    "    # Process API calls in parallel with progress bar\n",
    "    api_results = {}  # To store API results\n",
    "    api_call_counter = 0  # To count the number of API calls made\n",
    "\n",
    "    # Prepare output file path\n",
    "    output_file_path = 'data/output/patient_trial_matches.json'\n",
    "\n",
    "    # if api_call_tasks:\n",
    "    #     def process_api_call(task):\n",
    "    #         patient_id, trial_id, log_text = task\n",
    "    #         # Call the OpenAI API\n",
    "    #         api_result = call_openai(log_text)\n",
    "    #         # Return the result along with identifiers\n",
    "    #         return patient_id, trial_id, api_result\n",
    "\n",
    "    #     with ThreadPoolExecutor(max_workers=max_api_workers) as executor:\n",
    "    #         futures = [executor.submit(process_api_call, task) for task in api_call_tasks]\n",
    "    #         for future in tqdm(as_completed(futures), total=len(futures), desc=\"API Calls\"):\n",
    "    #             try:\n",
    "    #                 patient_id, trial_id, api_result = future.result()\n",
    "    #                 # Store the API result\n",
    "    #                 api_results[(patient_id, trial_id)] = api_result\n",
    "    #                 logger.info(f\"API call completed for patient {patient_id} and trial {trial_id}\")\n",
    "\n",
    "    #                 # Increment API call counter\n",
    "    #                 api_call_counter += 1\n",
    "\n",
    "    #                 # Every 1000 API calls, save the current results\n",
    "    #                 if api_call_counter % 1000 == 0 or api_call_counter == total_api_calls:\n",
    "    #                     logger.info(f\"Saving results after {api_call_counter} API calls.\")\n",
    "    #                     # Update logs and format output based on API results\n",
    "    #                     temp_results = update_logs_and_format_output(\n",
    "    #                         final_results=final_results,\n",
    "    #                         api_results=api_results,\n",
    "    #                         match_results_mapping=match_results_mapping,\n",
    "    #                         output_json_format=output_json_format,\n",
    "    #                         debug=debug\n",
    "    #                     )\n",
    "    #                     # Save the updated results to the JSON file\n",
    "    #                     save_results_to_json(temp_results, output_file_path, logger)\n",
    "\n",
    "    #             except Exception as e:\n",
    "    #                 logger.error(f\"API call failed: {e}\")\n",
    "\n",
    "    #     # After all API calls are done, ensure all results are saved\n",
    "    #     logger.info(\"All API calls completed. Saving final results.\")\n",
    "    #     final_results = update_logs_and_format_output(\n",
    "    #         final_results=final_results,\n",
    "    #         api_results=api_results,\n",
    "    #         match_results_mapping=match_results_mapping,\n",
    "    #         output_json_format=output_json_format,\n",
    "    #         debug=debug\n",
    "    #     )\n",
    "        # save_results_to_json(final_results, output_file_path, logger)\n",
    "\n",
    "    # else:\n",
    "    #     logger.info(\"No API calls needed.\")\n",
    "    #     # Save the final results without API updates\n",
    "    #     save_results_to_json(final_results, output_file_path, logger)\n",
    "    \n",
    "    \n",
    "    return final_results\n",
    "\n",
    "def evaluate_patient_trial_match_w_api(row, code_definitions_df, debug=False):\n",
    "    \"\"\"\n",
    "    Evaluate if a patient matches a trial's eligibility criteria with detailed messages.\n",
    "\n",
    "    Parameters:\n",
    "        row (pd.Series): Series containing patient and trial data.\n",
    "        code_definitions_df (pd.DataFrame): DataFrame with code definitions.\n",
    "        debug (bool): If True, generate logs for the match evaluation.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing match status, detailed criteria messages, and log text.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    import numpy as np\n",
    "\n",
    "    # print('ROw:', row)\n",
    "\n",
    "    # Extract patient and trial data from the row\n",
    "    patient_id = row['PatientID']\n",
    "    patient_age = row['Age']\n",
    "    patient_gender = row['Gender']\n",
    "    is_healthy = is_healthy_volunteer_row(row)\n",
    "    trial_id = row['NCTId']\n",
    "    trial_title = row['Title']\n",
    "    min_age = row['Minimum_Age']\n",
    "    max_age = row['Maximum_Age']\n",
    "    trial_sex = row['Sex']\n",
    "    healthy_volunteers = row['Healthy_Volunteers']\n",
    "    inclusion_criteria = row['Inclusion_Criteria']\n",
    "    exclusion_criteria = row['Exclusion_Criteria']\n",
    "    inclusion_codes_mapping = row['Inclusion_Criteria_UMLS_Codes']\n",
    "    exclusion_codes_mapping = row['Exclusion_Criteria_UMLS_Codes']\n",
    "    condition_codes = row['Condition_UMLS_CODES']\n",
    "    medication_codes = row['Medication_UMLS_CODES']\n",
    "    condition_desc_map = row['Condition_Code_Description_Map']\n",
    "    medication_desc_map = row['Medication_Code_Description_Map']\n",
    "\n",
    "    # Initialize match status and eligibility criteria messages\n",
    "    is_match = True\n",
    "    eligibility_criteria_met = []\n",
    "    eligibility_criteria_not_met = []\n",
    "\n",
    "    # Initialize logs\n",
    "    log_lines = []\n",
    "\n",
    "    # 1. Age check\n",
    "    # print('MIN AGE:', min_age, ', MAX AGE:', max_age)\n",
    "    min_age_display = min_age if not np.isnan(min_age) else 'Not Specified'\n",
    "    max_age_display = max_age if not np.isnan(max_age) else 'Not Specified'\n",
    "    age_match = is_age_match(patient_age, min_age, max_age)\n",
    "    if age_match:\n",
    "        message = f\"Patient Age ({patient_age}) matches Expected Range ({min_age_display}, {max_age_display})\"\n",
    "        eligibility_criteria_met.append(message)\n",
    "    else:\n",
    "        message = f\"Patient Age ({patient_age}) does not match Expected Range ({min_age_display}, {max_age_display})\"\n",
    "        eligibility_criteria_not_met.append(message)\n",
    "        is_match = False\n",
    "    log_lines.append(f\"1. {message}\")\n",
    "\n",
    "    # 2. Sex check\n",
    "    trial_sex_display = trial_sex if trial_sex else 'Not Specified'\n",
    "    sex_match = is_sex_match(patient_gender, trial_sex)\n",
    "    if sex_match:\n",
    "        message = f\"Patient Sex ({patient_gender}) matches Trial Sex Requirement ({trial_sex_display})\"\n",
    "        eligibility_criteria_met.append(message)\n",
    "    else:\n",
    "        message = f\"Patient Sex ({patient_gender}) does not match Trial Sex Requirement ({trial_sex_display})\"\n",
    "        eligibility_criteria_not_met.append(message)\n",
    "        is_match = False\n",
    "    log_lines.append(f\"2. {message}\")\n",
    "\n",
    "    # 3. Health status check\n",
    "    health_status_display = 'Healthy Volunteers' if healthy_volunteers else 'Patients'\n",
    "    health_match = True\n",
    "    if healthy_volunteers and not is_healthy:\n",
    "        health_match = False\n",
    "    if health_match:\n",
    "        message = f\"Patient Health Status ({'Healthy' if is_healthy else 'Not Healthy'}) matches Trial Requirement ({health_status_display})\"\n",
    "        eligibility_criteria_met.append(message)\n",
    "    else:\n",
    "        message = f\"Patient Health Status ({'Healthy' if is_healthy else 'Not Healthy'}) does not match Trial Requirement ({health_status_display})\"\n",
    "        eligibility_criteria_not_met.append(message)\n",
    "        is_match = False\n",
    "    log_lines.append(f\"3. {message}\")\n",
    "\n",
    "    # Initialize variable to indicate if we need to call API\n",
    "    need_api_call = False\n",
    "\n",
    "    # Proceed to Exclusion Criteria check\n",
    "    exclusion_match = True\n",
    "    exclusion_logs = []\n",
    "    num_exclusion_reasons = 0\n",
    "    overlapping_codes = set()\n",
    "\n",
    "    # Patient codes as sets\n",
    "    patient_condition_codes = set(condition_codes)\n",
    "    patient_medication_codes = set(medication_codes)\n",
    "\n",
    "    # Iterate over exclusion criteria\n",
    "    for idx, crit in enumerate(exclusion_criteria):\n",
    "        criterion_match = True  # Assume criterion matches\n",
    "        criterion_logs = [f\"   - Criterion {idx+1}: {crit}\"]\n",
    "\n",
    "        # Get codes associated with the criterion\n",
    "        criterion_codes = set(exclusion_codes_mapping.get(crit, []))\n",
    "\n",
    "        # Check condition codes\n",
    "        overlapping_condition_codes = patient_condition_codes.intersection(criterion_codes)\n",
    "        if overlapping_condition_codes:\n",
    "            criterion_match = False\n",
    "            num_exclusion_reasons += 1\n",
    "            overlapping_codes.update(overlapping_condition_codes)\n",
    "            for code in overlapping_condition_codes:\n",
    "                descriptions = condition_desc_map.get(code, [])\n",
    "                descriptions_str = ', '.join(descriptions) if descriptions else ''\n",
    "                criterion_logs.append(f\"      + Overlapping Condition Code [{code}]: {descriptions_str}\")\n",
    "\n",
    "        # Check medication codes\n",
    "        overlapping_medication_codes = patient_medication_codes.intersection(criterion_codes)\n",
    "        if overlapping_medication_codes:\n",
    "            criterion_match = False\n",
    "            num_exclusion_reasons += 1\n",
    "            overlapping_codes.update(overlapping_medication_codes)\n",
    "            for code in overlapping_medication_codes:\n",
    "                descriptions = medication_desc_map.get(code, [])\n",
    "                descriptions_str = ', '.join(descriptions) if descriptions else ''\n",
    "                criterion_logs.append(f\"      + Overlapping Medication Code [{code}]: {descriptions_str}\")\n",
    "\n",
    "        # Append match status for the criterion\n",
    "        if not criterion_match:\n",
    "            message = f\"Patient has overlapping conditions/medications with exclusion criterion: {crit}\"\n",
    "            eligibility_criteria_not_met.append(message)\n",
    "            exclusion_match = False\n",
    "        else:\n",
    "            message = f\"Patient has no overlapping conditions/medications with exclusion criterion: {crit}\"\n",
    "            eligibility_criteria_met.append(message)\n",
    "\n",
    "        criterion_logs[0] += f\" = {'MATCH' if criterion_match else 'NO MATCH'}\"\n",
    "        exclusion_logs.extend(criterion_logs)\n",
    "\n",
    "    log_lines.append(\"4. Exclusion Criteria:\")\n",
    "    log_lines.extend(exclusion_logs)\n",
    "\n",
    "    if exclusion_match:\n",
    "        log_lines.append(\"   - Exclusion Criteria Check: MATCH\")\n",
    "        # Since there is a match, we need to call the API to fact-check\n",
    "        need_api_call = True\n",
    "    else:\n",
    "        log_lines.append(\"   - Exclusion Criteria Check: NO MATCH\")\n",
    "        if num_exclusion_reasons < 2:\n",
    "            # Less than 2 reasons for exclusion, call the API to fact-check\n",
    "            need_api_call = True\n",
    "        else:\n",
    "            need_api_call = False\n",
    "        is_match = False  # Patient is not eligible due to exclusion criteria\n",
    "\n",
    "    # Combine log lines into 'log_text'\n",
    "    log_text = '\\n'.join(log_lines)\n",
    "\n",
    "    # If debug is True, write logs to file\n",
    "    if debug:\n",
    "        # Prepare patient-specific log directory\n",
    "        patient_log_dir = os.path.join('logs', patient_id)\n",
    "        os.makedirs(patient_log_dir, exist_ok=True)\n",
    "\n",
    "        # Prepare log file name\n",
    "        datetime_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        log_filename = f\"{trial_id}_{datetime_str}.log\"\n",
    "        if is_match:\n",
    "            log_filename = f\"[MATCH]_{log_filename}\"\n",
    "\n",
    "        # Write logs to file\n",
    "        log_filepath = os.path.join(patient_log_dir, log_filename)\n",
    "        with open(log_filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(log_text)\n",
    "\n",
    "    # Return the updated match result dictionary\n",
    "    return {\n",
    "        'is_match': is_match,\n",
    "        'eligibilityCriteriaMet': eligibility_criteria_met,\n",
    "        'eligibilityCriteriaNotMet': eligibility_criteria_not_met,\n",
    "        'need_api_call': need_api_call,\n",
    "        'log_text': log_text\n",
    "    }\n",
    "\n",
    "def perform_exclusion_criteria_check_row_w_api(row):\n",
    "    \"\"\"\n",
    "    Check exclusion criteria for a patient-trial pair.\n",
    "\n",
    "    Parameters:\n",
    "        row (pd.Series): Series containing patient and trial data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (exclusion_match (bool), exclusion_logs (list), num_exclusion_reasons (int), overlapping_codes (set))\n",
    "    \"\"\"\n",
    "    # Initialize match status\n",
    "    exclusion_match = True  # Assume match until proven otherwise\n",
    "    exclusion_logs = []\n",
    "    num_exclusion_reasons = 0\n",
    "    overlapping_codes = set()  # To collect overlapping UMLS codes\n",
    "\n",
    "    # Extract necessary data from row\n",
    "    exclusion_criteria = row['Exclusion_Criteria']\n",
    "    exclusion_codes_mapping = row['Exclusion_Criteria_UMLS_Codes']\n",
    "    condition_codes = set(row['Condition_UMLS_CODES'])\n",
    "    medication_codes = set(row['Medication_UMLS_CODES'])\n",
    "    condition_desc_map = row['Condition_Code_Description_Map']\n",
    "    medication_desc_map = row['Medication_Code_Description_Map']\n",
    "\n",
    "    # Iterate over exclusion criteria\n",
    "    for idx, crit in enumerate(exclusion_criteria):\n",
    "        criterion_match = True  # Assume criterion matches\n",
    "        criterion_logs = [f\"   - Criterion {idx+1}: {crit}\"]\n",
    "\n",
    "        # Get codes associated with the criterion\n",
    "        criterion_codes = set(exclusion_codes_mapping.get(crit, []))\n",
    "\n",
    "        # Check for overlapping condition codes\n",
    "        overlapping_condition_codes = condition_codes.intersection(criterion_codes)\n",
    "        if overlapping_condition_codes:\n",
    "            criterion_match = False\n",
    "            num_exclusion_reasons += 1\n",
    "            overlapping_codes.update(overlapping_condition_codes)\n",
    "            for code in overlapping_condition_codes:\n",
    "                descriptions = condition_desc_map.get(code, [])\n",
    "                descriptions_str = ', '.join(descriptions) if descriptions else ''\n",
    "                criterion_logs.append(f\"      + [{code}] {descriptions_str}\")\n",
    "\n",
    "        # Check for overlapping medication codes\n",
    "        overlapping_medication_codes = medication_codes.intersection(criterion_codes)\n",
    "        if overlapping_medication_codes:\n",
    "            criterion_match = False\n",
    "            num_exclusion_reasons += 1\n",
    "            overlapping_codes.update(overlapping_medication_codes)\n",
    "            for code in overlapping_medication_codes:\n",
    "                descriptions = medication_desc_map.get(code, [])\n",
    "                descriptions_str = ', '.join(descriptions) if descriptions else ''\n",
    "                criterion_logs.append(f\"      + [{code}] {descriptions_str}\")\n",
    "\n",
    "        # Append match status for the criterion\n",
    "        criterion_logs[0] += f\" = {'MATCH' if criterion_match else 'NO MATCH'}\"\n",
    "        if not criterion_match:\n",
    "            exclusion_match = False\n",
    "\n",
    "        exclusion_logs.extend(criterion_logs)\n",
    "\n",
    "    return exclusion_match, exclusion_logs, num_exclusion_reasons, overlapping_codes\n",
    "\n",
    "def evaluate_patient_trial_match(row, debug=False):\n",
    "    \"\"\"\n",
    "    Evaluate if a patient matches a trial's eligibility criteria.\n",
    "\n",
    "    Parameters:\n",
    "        row (pd.Series): Series containing patient and trial data.\n",
    "        debug (bool): If True, generate logs for the match evaluation.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing match status and criteria met.\n",
    "    \"\"\"\n",
    "    # Extract patient and trial data from the row\n",
    "    patient_id = row['PatientID']\n",
    "    patient_age = row['Age']\n",
    "    patient_gender = row['Gender']\n",
    "    is_healthy = is_healthy_volunteer_row(row)\n",
    "    trial_id = row['NCTId']\n",
    "    trial_title = row['Title']\n",
    "    min_age = row['Minimum_Age']\n",
    "    max_age = row['Maximum_Age']\n",
    "    trial_sex = row['Sex']\n",
    "    healthy_volunteers = row['Healthy_Volunteers']\n",
    "\n",
    "    # Initialize match status and eligibility criteria met\n",
    "    is_match = True\n",
    "    eligibility_criteria_met = []\n",
    "\n",
    "    # Initialize logs if debug is True\n",
    "    log_lines = []\n",
    "    if debug:\n",
    "        log_lines.append(f\"Patient ID: {patient_id}\")\n",
    "        log_lines.append(f\"Trial ID: {trial_id}\")\n",
    "        log_lines.append(\"Match Check:\")\n",
    "\n",
    "    # 1. Age check\n",
    "    age_match = is_age_match(patient_age, min_age, max_age)\n",
    "    if age_match:\n",
    "        eligibility_criteria_met.append('Age')\n",
    "    else:\n",
    "        is_match = False\n",
    "    if debug:\n",
    "        log_lines.append(f\"1. Age = {patient_age} vs [{min_age}, {max_age}] = {'MATCH' if age_match else 'NO MATCH'}\")\n",
    "\n",
    "    # 2. Sex check\n",
    "    sex_match = is_sex_match(patient_gender, trial_sex)\n",
    "    if sex_match:\n",
    "        eligibility_criteria_met.append('Sex')\n",
    "    else:\n",
    "        is_match = False\n",
    "    if debug:\n",
    "        log_lines.append(f\"2. Sex = {patient_gender} vs {trial_sex} = {'MATCH' if sex_match else 'NO MATCH'}\")\n",
    "\n",
    "    # 3. Health check\n",
    "    health_match = True\n",
    "    if healthy_volunteers and not is_healthy:\n",
    "        health_match = False\n",
    "    if health_match:\n",
    "        eligibility_criteria_met.append('Health Status')\n",
    "    else:\n",
    "        is_match = False\n",
    "    if debug:\n",
    "        log_lines.append(f\"3. Health = {'Healthy' if is_healthy else 'Not Healthy'} vs {'Healthy Volunteers' if healthy_volunteers else 'Patients'} = {'MATCH' if health_match else 'NO MATCH'}\")\n",
    "\n",
    "    # Proceed only if basic checks passed\n",
    "    if is_match:\n",
    "        # 4. Exclusion Criteria check\n",
    "        exclusion_match, exclusion_logs = perform_exclusion_criteria_check_row(row)\n",
    "        if exclusion_match:\n",
    "            eligibility_criteria_met.append('Exclusion Criteria')\n",
    "        else:\n",
    "            is_match = False\n",
    "        if debug:\n",
    "            log_lines.append(\"4. Exclusion Criteria:\")\n",
    "            log_lines.extend(exclusion_logs)\n",
    "\n",
    "    # Generate logs if debug is True\n",
    "    if debug:\n",
    "        # Prepare patient-specific log directory\n",
    "        patient_log_dir = os.path.join('logs', patient_id)\n",
    "        os.makedirs(patient_log_dir, exist_ok=True)\n",
    "\n",
    "        # Prepare log file name\n",
    "        datetime_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        log_filename = f\"{trial_id}_{datetime_str}.log\"\n",
    "        if is_match:\n",
    "            log_filename = f\"[MATCH]_{log_filename}\"\n",
    "\n",
    "        # Write logs to file\n",
    "        log_filepath = os.path.join(patient_log_dir, log_filename)\n",
    "        with open(log_filepath, 'w', encoding='utf-8') as f:\n",
    "            for line in log_lines:\n",
    "                f.write(line + '\\n')\n",
    "\n",
    "    return {'is_match': is_match, 'eligibilityCriteriaMet': eligibility_criteria_met}\n",
    "\n",
    "def is_healthy_volunteer_row(row):\n",
    "    \"\"\"\n",
    "    Determine if a patient is a healthy volunteer based on row data.\n",
    "    \"\"\"\n",
    "    return len(row['Condition_Description']) == 0\n",
    "\n",
    "def perform_exclusion_criteria_check_row(row):\n",
    "    \"\"\"\n",
    "    Check exclusion criteria for a patient-trial pair.\n",
    "\n",
    "    Parameters:\n",
    "        row (pd.Series): Series containing patient and trial data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (exclusion_match (bool), exclusion_logs (list))\n",
    "    \"\"\"\n",
    "    exclusion_match = True  # Assume match until proven otherwise\n",
    "    exclusion_logs = []\n",
    "\n",
    "    # Patient codes\n",
    "    patient_condition_codes = set(row['Condition_UMLS_CODES'])\n",
    "    patient_medication_codes = set(row['Medication_UMLS_CODES'])\n",
    "\n",
    "    # Iterate over exclusion criteria\n",
    "    exclusion_criteria = row['Exclusion_Criteria']\n",
    "    exclusion_codes_mapping = row['Exclusion_Criteria_UMLS_Codes']\n",
    "\n",
    "    for idx, crit in enumerate(exclusion_criteria):\n",
    "        criterion_match = True  # Assume criterion matches\n",
    "        criterion_logs = [f\"   - Criterion {idx+1}: {crit}\"]\n",
    "\n",
    "        # Get codes associated with the criterion\n",
    "        criterion_codes = set(exclusion_codes_mapping.get(crit, []))\n",
    "\n",
    "        # Check condition codes\n",
    "        overlapping_condition_codes = patient_condition_codes.intersection(criterion_codes)\n",
    "        if overlapping_condition_codes:\n",
    "            criterion_match = False\n",
    "            for code in overlapping_condition_codes:\n",
    "                descriptions = row['Condition_Code_Description_Map'].get(code, [])\n",
    "                descriptions_str = ', '.join(descriptions)\n",
    "                criterion_logs.append(f\"      + [{code}] {descriptions_str}\")\n",
    "\n",
    "        # Check medication codes\n",
    "        overlapping_medication_codes = patient_medication_codes.intersection(criterion_codes)\n",
    "        if overlapping_medication_codes:\n",
    "            criterion_match = False\n",
    "            for code in overlapping_medication_codes:\n",
    "                descriptions = row['Medication_Code_Description_Map'].get(code, [])\n",
    "                descriptions_str = ', '.join(descriptions)\n",
    "                criterion_logs.append(f\"      + [{code}] {descriptions_str}\")\n",
    "\n",
    "        # Append match status for the criterion\n",
    "        criterion_logs[0] += f\" = {'MATCH' if criterion_match else 'NO MATCH'}\"\n",
    "        if not criterion_match:\n",
    "            exclusion_match = False\n",
    "\n",
    "        exclusion_logs.extend(criterion_logs)\n",
    "\n",
    "    return exclusion_match, exclusion_logs\n",
    "\n",
    "def prepare_trial_data(df_trials):\n",
    "    \"\"\"\n",
    "    Preprocess trial data to ensure proper data types.\n",
    "    \"\"\"\n",
    "    # Convert age fields to integers\n",
    "    df_trials['Minimum_Age'] = df_trials['Minimum_Age'].apply(convert_age)\n",
    "    df_trials['Maximum_Age'] = df_trials['Maximum_Age'].apply(convert_age)\n",
    "    # Ensure 'Healthy_Volunteers' is boolean\n",
    "    df_trials['Healthy_Volunteers'] = df_trials['Healthy_Volunteers'].apply(lambda x: x.lower() == 'yes' if isinstance(x, str) else False)\n",
    "    return df_trials\n",
    "\n",
    "def convert_age(age_str):\n",
    "    \"\"\"\n",
    "    Convert age strings to integers.\n",
    "    \"\"\"\n",
    "    # if age is not str\n",
    "    if not isinstance(age_str, str):\n",
    "        return age_str\n",
    "    if pd.isna(age_str) or age_str in ['Not Specified', 'N/A']:\n",
    "        return np.nan\n",
    "    try:\n",
    "        return int(age_str.split()[0])\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def is_age_match(patient_age, min_age, max_age):\n",
    "    \"\"\"\n",
    "    Check if patient's age matches trial's age criteria.\n",
    "    \"\"\"\n",
    "    if pd.isna(min_age):\n",
    "        min_age = 0\n",
    "    if pd.isna(max_age):\n",
    "        max_age = 120\n",
    "    return min_age <= patient_age <= max_age\n",
    "\n",
    "def is_sex_match(patient_gender, trial_gender):\n",
    "    \"\"\"\n",
    "    Check if patient's gender matches trial's sex criteria.\n",
    "    \"\"\"\n",
    "    if trial_gender.upper() == \"ALL\":\n",
    "        return True\n",
    "    return patient_gender.upper() == trial_gender.upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_trials = fetch_n_trials(1000)\n",
    "df_trials = fetch_n_trials_v2(n=None, max_workers=8, verbose=True, reload=False)\n",
    "# ran on 10/4/2024\n",
    "# took 438mins to run which is 7.3 hours\n",
    "# processing with lg model took most time. # can use md model or sm model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare patient data \n",
    "from datetime import datetime\n",
    "# using reload will take 2-3 minutes to reload data and annotate, use this only if the synthea data has changed due to scaling\n",
    "patient_profiles_grouped = prepare_patient_data(reload=False)\n",
    "\n",
    "# Fetch trials and prepare trial data\n",
    "# df_trials = fetch_n_trials(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_trials load the codes column as it curr string, convert to dict\n",
    "# 1 min to run for 67k trials\n",
    "df_trials['Inclusion_Criteria_UMLS_Codes'] = df_trials['Inclusion_Criteria_UMLS_Codes'].apply(lambda x: eval(x))\n",
    "df_trials['Exclusion_Criteria_UMLS_Codes'] = df_trials['Exclusion_Criteria_UMLS_Codes'].apply(lambda x: eval(x))\n",
    "df_trials['Inclusion_Criteria'] = df_trials['Inclusion_Criteria'].apply(lambda x: eval(x))\n",
    "df_trials['Exclusion_Criteria'] = df_trials['Exclusion_Criteria'].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Collect UMLS codes from patient data\n",
    "patient_condition_codes = set(patient_profiles_grouped['Condition_UMLS_CODES'].explode())\n",
    "patient_medication_codes = set(patient_profiles_grouped['Medication_UMLS_CODES'].explode())\n",
    "\n",
    "# Collect UMLS codes from trial data\n",
    "trial_inclusion_codes = set()\n",
    "trial_exclusion_codes = set()\n",
    "\n",
    "for codes_mapping in df_trials['Inclusion_Criteria_UMLS_Codes']:\n",
    "    for codes in codes_mapping.values():\n",
    "        trial_inclusion_codes.update(codes)\n",
    "\n",
    "for codes_mapping in df_trials['Exclusion_Criteria_UMLS_Codes']:\n",
    "    for codes in codes_mapping.values():\n",
    "        trial_exclusion_codes.update(codes)\n",
    "\n",
    "# Combine all unique codes\n",
    "all_unique_codes = patient_condition_codes.union(\n",
    "    patient_medication_codes,\n",
    "    trial_inclusion_codes,\n",
    "    trial_exclusion_codes\n",
    ")\n",
    "\n",
    "# Remove any NaN or None values\n",
    "all_unique_codes.discard(np.nan)\n",
    "all_unique_codes.discard(None)\n",
    "\n",
    "code_definitions_df = extract_umls_code_definitions(\n",
    "    unique_codes=all_unique_codes,\n",
    "    # nlp_umls_conditions=nlp_umls_link,\n",
    "    # nlp_umls_medications=nlp_rxnorm_link\n",
    "    nlp_umls_conditions=None,\n",
    "    nlp_umls_medications=None,\n",
    "    reload=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>canonical_name</th>\n",
       "      <th>definition</th>\n",
       "      <th>aliases</th>\n",
       "      <th>types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C0700087</td>\n",
       "      <td>Tegretol</td>\n",
       "      <td>a kind of epilepsy treatment drug</td>\n",
       "      <td>[tegretol, Tegretol, TEGretol]</td>\n",
       "      <td>[T109, T121]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C5555091</td>\n",
       "      <td>60 Months</td>\n",
       "      <td>A period of time of sixty months.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[T079]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C4085349</td>\n",
       "      <td>Medial collateral ligament</td>\n",
       "      <td>None</td>\n",
       "      <td>[Medial collateral ligament complex, Medial co...</td>\n",
       "      <td>[T023]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C0700201</td>\n",
       "      <td>Dyssomnias</td>\n",
       "      <td>A broad category of sleep disorders characteri...</td>\n",
       "      <td>[Dyssomnia, NOS, Dyssomnia (disorder), sleep d...</td>\n",
       "      <td>[T048]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1160964</td>\n",
       "      <td>Fracture care</td>\n",
       "      <td>Actions performed to control broken bones</td>\n",
       "      <td>[Fracture Care, care fractures, care fracture,...</td>\n",
       "      <td>[T061]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1888</th>\n",
       "      <td>C0030705</td>\n",
       "      <td>Patients</td>\n",
       "      <td>Individuals participating in the health care s...</td>\n",
       "      <td>[patients, patient, Patient, Patient (person),...</td>\n",
       "      <td>[T101]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889</th>\n",
       "      <td>C2827257</td>\n",
       "      <td>lisinopril anhydrous</td>\n",
       "      <td>The anhydrous form of the long-acting angioten...</td>\n",
       "      <td>[L-Proline, 1-(N(Sup 2)-(1-Carboxy-3-Phenylpro...</td>\n",
       "      <td>[T116, T121]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>C0242477</td>\n",
       "      <td>Refugee Camps</td>\n",
       "      <td>A temporary, often makeshift shelter for perso...</td>\n",
       "      <td>[Refugee Camp, refugee camps, Camps, Refugee, ...</td>\n",
       "      <td>[T082]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>C0439230</td>\n",
       "      <td>week</td>\n",
       "      <td>Any period of seven consecutive days.</td>\n",
       "      <td>[Weeks, week, Week, week (qualifier value), we...</td>\n",
       "      <td>[T079]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>C0032285</td>\n",
       "      <td>Pneumonia</td>\n",
       "      <td>Infection of the lung often accompanied by inf...</td>\n",
       "      <td>[Lung inflamed, Pulmonitis, PNEUMONIA, Pneumon...</td>\n",
       "      <td>[T047]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1893 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          code              canonical_name  \\\n",
       "0     C0700087                    Tegretol   \n",
       "1     C5555091                   60 Months   \n",
       "2     C4085349  Medial collateral ligament   \n",
       "3     C0700201                  Dyssomnias   \n",
       "4     C1160964               Fracture care   \n",
       "...        ...                         ...   \n",
       "1888  C0030705                    Patients   \n",
       "1889  C2827257        lisinopril anhydrous   \n",
       "1890  C0242477               Refugee Camps   \n",
       "1891  C0439230                        week   \n",
       "1892  C0032285                   Pneumonia   \n",
       "\n",
       "                                             definition  \\\n",
       "0                     a kind of epilepsy treatment drug   \n",
       "1                     A period of time of sixty months.   \n",
       "2                                                  None   \n",
       "3     A broad category of sleep disorders characteri...   \n",
       "4             Actions performed to control broken bones   \n",
       "...                                                 ...   \n",
       "1888  Individuals participating in the health care s...   \n",
       "1889  The anhydrous form of the long-acting angioten...   \n",
       "1890  A temporary, often makeshift shelter for perso...   \n",
       "1891              Any period of seven consecutive days.   \n",
       "1892  Infection of the lung often accompanied by inf...   \n",
       "\n",
       "                                                aliases         types  \n",
       "0                        [tegretol, Tegretol, TEGretol]  [T109, T121]  \n",
       "1                                                    []        [T079]  \n",
       "2     [Medial collateral ligament complex, Medial co...        [T023]  \n",
       "3     [Dyssomnia, NOS, Dyssomnia (disorder), sleep d...        [T048]  \n",
       "4     [Fracture Care, care fractures, care fracture,...        [T061]  \n",
       "...                                                 ...           ...  \n",
       "1888  [patients, patient, Patient, Patient (person),...        [T101]  \n",
       "1889  [L-Proline, 1-(N(Sup 2)-(1-Carboxy-3-Phenylpro...  [T116, T121]  \n",
       "1890  [Refugee Camp, refugee camps, Camps, Refugee, ...        [T082]  \n",
       "1891  [Weeks, week, Week, week (qualifier value), we...        [T079]  \n",
       "1892  [Lung inflamed, Pulmonitis, PNEUMONIA, Pneumon...        [T047]  \n",
       "\n",
       "[1893 rows x 5 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_umls_codes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the models to free up memory\n",
    "del nlp_umls_link\n",
    "del nlp_rxnorm_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_trials' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_trials\u001b[49m\u001b[38;5;241m.\u001b[39mHealthy_Volunteers\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_trials' is not defined"
     ]
    }
   ],
   "source": [
    "df_trials.Healthy_Volunteers.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:match_patients_to_trials_w_api:Trial data prepared.\n",
      "Patient Chunks:   0%|          | 0/1 [00:00<?, ?it/s]INFO:match_patients_to_trials_w_api:Processing patient chunk starting at index 100\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 0\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:02<00:00, 5112.19it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 1000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5586.57it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 2000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:02<00:00, 5349.00it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 3000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5681.15it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 4000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5741.68it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 5000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5876.48it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 6000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6209.20it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 7000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6034.98it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 8000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5668.72it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 9000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6238.47it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 10000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5941.89it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 11000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6363.84it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 12000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6024.53it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 13000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5888.97it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 14000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6285.53it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 15000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6374.25it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 16000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6673.59it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 17000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6184.35it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 18000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5957.31it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 19000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6082.44it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 20000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6142.11it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 21000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5775.56it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 22000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5892.13it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 23000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6106.03it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 24000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6066.79it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 25000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6086.65it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 26000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5967.51it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 27000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5945.19it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 28000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5912.06it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 29000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5980.36it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 30000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6027.40it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 31000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6145.41it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 32000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5962.85it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 33000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6059.82it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 34000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5947.75it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 35000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6052.50it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 36000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6361.18it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 37000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6326.14it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 38000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5938.28it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 39000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6009.76it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 40000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6073.73it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 41000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5993.47it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 42000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6132.90it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 43000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6025.99it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 44000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6216.82it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 45000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6259.01it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 46000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5870.95it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 47000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6060.01it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 48000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6022.85it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 49000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5975.75it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 50000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5798.41it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 51000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5868.24it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 52000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6139.81it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 53000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6057.34it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 54000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6063.13it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 55000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5828.10it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 56000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5973.18it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 57000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5780.35it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 58000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5666.93it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 59000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5818.75it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 60000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5814.16it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 61000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5963.96it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 62000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5969.35it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 63000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5846.75it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 64000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6313.75it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 65000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 6108.11it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 66000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 11000/11000 [00:01<00:00, 5924.59it/s]\n",
      "INFO:match_patients_to_trials_w_api:Processing trial chunk starting at index 67000\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 2409/2409 [00:00<00:00, 5471.84it/s]\n",
      "ERROR:match_patients_to_trials_w_api:Failed to save results to data/output/patient_trial_matches_chunk_100.json: [Errno 28] No space left on device\n",
      "Patient Chunks: 100%|██████████| 1/1 [03:06<00:00, 186.45s/it]\n",
      "INFO:match_patients_to_trials_w_api:All patient chunks processed. Returning final results.\n"
     ]
    }
   ],
   "source": [
    "# Call the function with desired parameters\n",
    "# go to this function and uncomment the code to call the api which should be at the end of the function\n",
    "results = match_patients_to_trials(\n",
    "    patient_profiles_grouped,\n",
    "    # send only 1 patient for testing\n",
    "    # patient_profiles_grouped.iloc[0:2],\n",
    "    df_trials,\n",
    "    # df_trials.iloc[:10],\n",
    "    code_definitions_df,\n",
    "    chunk_size=1000,\n",
    "    debug=False, # use this to save details patient-trial match logs containing\n",
    "    verbose=True,\n",
    "    max_api_workers=8,\n",
    "    output_json_format='match',  # Options: 'match', 'no_match', 'both'\n",
    "    patient_start_chunk=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results convert to df \n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_eligible_trials\n",
       "3812     1\n",
       "45599    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results_df len of eligible trials\n",
    "results_df['num_eligible_trials'] = results_df['eligibleTrials'].apply(len)\n",
    "results_df['num_eligible_trials'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patientId</th>\n",
       "      <th>eligibleTrials</th>\n",
       "      <th>num_eligible_trials</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03f12f9e-fd3e-b845-502a-3a12511d9e48</td>\n",
       "      <td>[{'trialId': 'NCT03668327', 'trialName': 'Pret...</td>\n",
       "      <td>3812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>068d2ed4-b12c-e380-fe78-3d99eab488c2</td>\n",
       "      <td>[{'trialId': 'NCT06286527', 'trialName': 'Qual...</td>\n",
       "      <td>45599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              patientId  \\\n",
       "0  03f12f9e-fd3e-b845-502a-3a12511d9e48   \n",
       "1  068d2ed4-b12c-e380-fe78-3d99eab488c2   \n",
       "\n",
       "                                      eligibleTrials  num_eligible_trials  \n",
       "0  [{'trialId': 'NCT03668327', 'trialName': 'Pret...                 3812  \n",
       "1  [{'trialId': 'NCT06286527', 'trialName': 'Qual...                45599  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "matplotlib is required for plotting when the default backend \"matplotlib\" is selected.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# histogram of no of patients by % of eligible trials total trials = 67219\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mresults_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_eligible_trials\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cx-admin\\Documents\\cx-ag\\ag_job_apps\\Turmerik\\TakeHome-ML-SWE\\.venv\\Lib\\site-packages\\pandas\\plotting\\_core.py:947\u001b[0m, in \u001b[0;36mPlotAccessor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 947\u001b[0m     plot_backend \u001b[38;5;241m=\u001b[39m \u001b[43m_get_plot_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    949\u001b[0m     x, y, kind, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_call_args(\n\u001b[0;32m    950\u001b[0m         plot_backend\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent, args, kwargs\n\u001b[0;32m    951\u001b[0m     )\n\u001b[0;32m    953\u001b[0m     kind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kind_aliases\u001b[38;5;241m.\u001b[39mget(kind, kind)\n",
      "File \u001b[1;32mc:\\Users\\cx-admin\\Documents\\cx-ag\\ag_job_apps\\Turmerik\\TakeHome-ML-SWE\\.venv\\Lib\\site-packages\\pandas\\plotting\\_core.py:1944\u001b[0m, in \u001b[0;36m_get_plot_backend\u001b[1;34m(backend)\u001b[0m\n\u001b[0;32m   1941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend_str \u001b[38;5;129;01min\u001b[39;00m _backends:\n\u001b[0;32m   1942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _backends[backend_str]\n\u001b[1;32m-> 1944\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43m_load_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1945\u001b[0m _backends[backend_str] \u001b[38;5;241m=\u001b[39m module\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[1;32mc:\\Users\\cx-admin\\Documents\\cx-ag\\ag_job_apps\\Turmerik\\TakeHome-ML-SWE\\.venv\\Lib\\site-packages\\pandas\\plotting\\_core.py:1874\u001b[0m, in \u001b[0;36m_load_backend\u001b[1;34m(backend)\u001b[0m\n\u001b[0;32m   1872\u001b[0m         module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas.plotting._matplotlib\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1873\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m-> 1874\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   1875\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatplotlib is required for plotting when the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1876\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault backend \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is selected.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1877\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m module\n\u001b[0;32m   1880\u001b[0m found_backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: matplotlib is required for plotting when the default backend \"matplotlib\" is selected."
     ]
    }
   ],
   "source": [
    "# histogram of no of patients by % of eligible trials total trials = 67219\n",
    "results_df['num_eligible_trials'].value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
